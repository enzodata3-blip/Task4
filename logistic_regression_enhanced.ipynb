{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression — Enhanced (Translated & Optimized)\n",
    "\n",
    "**Original source:** [Jack-Cherish/Machine-Learning](https://github.com/Jack-Cherish/Machine-Learning)  \n",
    "**Translated from Chinese and enhanced with advanced statistical analysis.**\n",
    "\n",
    "## What this notebook covers\n",
    "\n",
    "| Section | Content |\n",
    "|---|---|\n",
    "| 1 | Gradient ascent demo (find maximum of f(x) = −x² + 4x) |\n",
    "| 2 | Logistic regression on 2D synthetic data (batch GD + L2 regularization) |\n",
    "| 3 | Horse colic survival prediction (UCI dataset, 21 clinical features) |\n",
    "| 4 | Correlation matrix → interaction term selection |\n",
    "| 5 | Custom SGD vs sklearn — full metric comparison |\n",
    "| 6 | Stratified k-fold cross-validation |\n",
    "\n",
    "## Enhancements over the original\n",
    "- Full English translation of all code, comments, and output\n",
    "- Feature standardization (zero mean, unit variance) — prevents gradient scale issues\n",
    "- L2 regularization in batch gradient ascent\n",
    "- Interaction terms derived from correlation matrix analysis\n",
    "- Comprehensive evaluation: confusion matrix, accuracy, precision, recall, specificity, F1, AUC-ROC, log-loss, MCC\n",
    "- Convergence monitoring via log-loss curve\n",
    "- Stratified k-fold cross-validation with mean ± std reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    log_loss, matthews_corrcoef, roc_curve\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Gradient Ascent Demo\n",
    "\n",
    "Illustrates the gradient ascent update rule by finding the maximum of:\n",
    "\n",
    "$$f(x) = -x^2 + 4x$$\n",
    "\n",
    "The derivative $f'(x) = -2x + 4 = 0$ at $x = 2$, so we expect the algorithm to converge near 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent_demo():\n",
    "    \"\"\"Find the maximum of f(x) = -x^2 + 4x using gradient ascent.\"\"\"\n",
    "    def f_prime(x):\n",
    "        return -2 * x + 4\n",
    "\n",
    "    x_old = -1.0          # initial guess (must differ from x_new to enter loop)\n",
    "    x_new = 0.0           # starting point\n",
    "    learning_rate = 0.01  # controls update magnitude\n",
    "    precision = 1e-8      # convergence threshold\n",
    "\n",
    "    iterations = 0\n",
    "    while abs(x_new - x_old) > precision:\n",
    "        x_old = x_new\n",
    "        x_new = x_old + learning_rate * f_prime(x_old)\n",
    "        iterations += 1\n",
    "\n",
    "    print(f\"Converged in {iterations} iterations\")\n",
    "    print(f\"Maximum of f(x) = -x² + 4x found at x = {x_new:.8f}  (expected: 2.0)\")\n",
    "    print(f\"f({x_new:.4f}) = {-x_new**2 + 4*x_new:.6f}  (expected: 4.0)\")\n",
    "\n",
    "gradient_ascent_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Logistic Regression on 2D Synthetic Data\n",
    "\n",
    "Shared helper functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sigmoid ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid: avoids overflow for large |z|.\"\"\"\n",
    "    return np.where(z >= 0,\n",
    "                    1.0 / (1.0 + np.exp(-z)),\n",
    "                    np.exp(z) / (1.0 + np.exp(z)))\n",
    "\n",
    "\n",
    "# ── Evaluation suite ─────────────────────────────────────────────────────────\n",
    "\n",
    "def evaluate(y_true, y_pred, y_prob, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"Print a comprehensive suite of classification metrics.\n",
    "\n",
    "    Reports: confusion matrix, accuracy, precision, recall, specificity,\n",
    "    F1, AUC-ROC, log-loss, Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [\"Class 0\", \"Class 1\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Evaluation — {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"              Predicted 0   Predicted 1\")\n",
    "    print(f\"  Actual 0       {tn:6d}        {fp:6d}\")\n",
    "    print(f\"  Actual 1       {fn:6d}        {tp:6d}\")\n",
    "\n",
    "    accuracy    = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision   = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall      = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    f1          = (2 * precision * recall / (precision + recall)\n",
    "                   if (precision + recall) > 0 else 0.0)\n",
    "    mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "    auc  = roc_auc_score(y_true, y_prob)\n",
    "    ll   = log_loss(y_true, y_prob)\n",
    "\n",
    "    print(f\"\\nCore Metrics:\")\n",
    "    print(f\"  Accuracy    : {accuracy:.4f}  ({(1-accuracy)*100:.2f}% error rate)\")\n",
    "    print(f\"  Precision   : {precision:.4f}  (TP / (TP+FP))\")\n",
    "    print(f\"  Recall      : {recall:.4f}  (sensitivity, TP / (TP+FN))\")\n",
    "    print(f\"  Specificity : {specificity:.4f}  (TN / (TN+FP))\")\n",
    "    print(f\"  F1-Score    : {f1:.4f}\")\n",
    "    print(f\"\\nAdvanced Metrics:\")\n",
    "    print(f\"  AUC-ROC     : {auc:.4f}\")\n",
    "    print(f\"  Log-Loss    : {ll:.4f}\")\n",
    "    print(f\"  MCC         : {mcc:.4f}  (ranges -1 to +1, 0 = random)\")\n",
    "    print(f\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    return dict(accuracy=accuracy, precision=precision, recall=recall,\n",
    "                specificity=specificity, f1=f1, auc_roc=auc,\n",
    "                log_loss=ll, mcc=mcc)\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the 2D synthetic dataset (`testSet.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2d_dataset(filepath=\"testSet.txt\"):\n",
    "    \"\"\"Load 2D binary dataset.  Format: x1  x2  label (space-separated).\n",
    "    Prepends a bias column of 1.0.\n",
    "    \"\"\"\n",
    "    data_matrix, labels = [], []\n",
    "    with open(filepath) as fh:\n",
    "        for line in fh.readlines():\n",
    "            parts = line.strip().split()\n",
    "            data_matrix.append([1.0, float(parts[0]), float(parts[1])])\n",
    "            labels.append(int(parts[2]))\n",
    "    return np.array(data_matrix), np.array(labels)\n",
    "\n",
    "\n",
    "def standardize_2d(X):\n",
    "    \"\"\"Standardize feature columns (skip bias at index 0).\"\"\"\n",
    "    arr = X.copy().astype(float)\n",
    "    means = arr[:, 1:].mean(axis=0)\n",
    "    stds  = arr[:, 1:].std(axis=0)\n",
    "    stds[stds == 0] = 1.0\n",
    "    arr[:, 1:] = (arr[:, 1:] - means) / stds\n",
    "    return arr, means, stds\n",
    "\n",
    "\n",
    "X_2d, y_2d = load_2d_dataset(\"testSet.txt\")\n",
    "X_2d_std, _, _ = standardize_2d(X_2d)\n",
    "\n",
    "print(f\"Dataset shape : {X_2d.shape}\")\n",
    "print(f\"Class balance : {(y_2d==0).sum()} class-0  /  {(y_2d==1).sum()} class-1\")\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(X_2d[y_2d==1, 1], X_2d[y_2d==1, 2], s=25, c='red',   marker='s', alpha=0.7, label='Class 1')\n",
    "ax.scatter(X_2d[y_2d==0, 1], X_2d[y_2d==0, 2], s=25, c='green',             alpha=0.7, label='Class 0')\n",
    "ax.set_title('2D Synthetic Dataset'); ax.set_xlabel('X1'); ax.set_ylabel('X2'); ax.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Batch gradient ascent with L2 regularization\n",
    "\n",
    "Maximises the regularized log-likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\sum_i \\left[ y_i \\log h_i + (1-y_i) \\log(1-h_i) \\right] - \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_ascent(X, y, learning_rate=0.005, max_iter=1000, l2_lambda=0.01):\n",
    "    \"\"\"Batch gradient ascent with optional L2 regularization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X           : array (m, n) — includes bias column\n",
    "    y           : array (m,)\n",
    "    learning_rate : float\n",
    "    max_iter    : int\n",
    "    l2_lambda   : float — L2 penalty strength (0 = disabled)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights      : ndarray (n,)\n",
    "    loss_history : list[float]\n",
    "    \"\"\"\n",
    "    Xm = np.mat(X, dtype=float)\n",
    "    ym = np.mat(y, dtype=float).T\n",
    "    m, n = Xm.shape\n",
    "    w = np.ones((n, 1))\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        h = sigmoid(Xm * w)\n",
    "        error = ym - h\n",
    "        penalty = np.zeros_like(w)\n",
    "        penalty[1:] = l2_lambda * w[1:]          # do not penalize bias\n",
    "        w = w + learning_rate * (Xm.T * error - penalty)\n",
    "\n",
    "        h_arr = np.asarray(h).ravel()\n",
    "        y_arr = np.asarray(ym).ravel()\n",
    "        loss_history.append(log_loss(y_arr, h_arr))\n",
    "\n",
    "    return np.asarray(w).ravel(), loss_history\n",
    "\n",
    "\n",
    "weights_batch, loss_hist_batch = batch_gradient_ascent(\n",
    "    X_2d_std, y_2d, learning_rate=0.005, max_iter=1000, l2_lambda=0.01\n",
    ")\n",
    "print(f\"Final weights: {weights_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convergence curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(loss_hist_batch, color='steelblue', lw=1.5)\n",
    "plt.xlabel('Iteration'); plt.ylabel('Log-Loss')\n",
    "plt.title('Convergence — Batch Gradient Ascent (L2 regularized)')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation + decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_batch = sigmoid(X_2d_std @ weights_batch)\n",
    "y_pred_batch = (y_prob_batch >= 0.5).astype(int)\n",
    "\n",
    "metrics_batch = evaluate(y_2d, y_pred_batch, y_prob_batch,\n",
    "                         model_name=\"Logistic Regression — Batch GD (2D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary: w0 + w1*x1 + w2*x2 = 0  =>  x2 = -(w0 + w1*x1) / w2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: decision boundary\n",
    "ax = axes[0]\n",
    "ax.scatter(X_2d_std[y_2d==1, 1], X_2d_std[y_2d==1, 2], s=25, c='red',   marker='s', alpha=0.7, label='Class 1')\n",
    "ax.scatter(X_2d_std[y_2d==0, 1], X_2d_std[y_2d==0, 2], s=25, c='green',             alpha=0.7, label='Class 0')\n",
    "x1r = np.linspace(X_2d_std[:, 1].min()-0.3, X_2d_std[:, 1].max()+0.3, 200)\n",
    "x2b = -(weights_batch[0] + weights_batch[1] * x1r) / weights_batch[2]\n",
    "ax.plot(x1r, x2b, 'b-', lw=2, label='Decision boundary')\n",
    "ax.set_title('Decision Boundary (Standardized)'); ax.set_xlabel('X1'); ax.set_ylabel('X2'); ax.legend()\n",
    "\n",
    "# Right: ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_2d, y_prob_batch)\n",
    "auc_val = roc_auc_score(y_2d, y_prob_batch)\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC={auc_val:.3f})')\n",
    "axes[1].plot([0,1],[0,1],'k--', lw=1, label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate'); axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve — Batch GD'); axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Horse Colic Survival Prediction (UCI Dataset)\n",
    "\n",
    "Binary classification on 21 clinical veterinary features.  \n",
    "**Label:** 0 = did not survive, 1 = survived.\n",
    "\n",
    "### 3.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names translated from original UCI documentation\n",
    "FEATURE_NAMES = [\n",
    "    \"surgery\",                   # 1=yes, 2=no\n",
    "    \"age\",                       # 1=adult, 2=young\n",
    "    \"rectal_temp\",               # degrees Celsius\n",
    "    \"pulse\",                     # beats per minute\n",
    "    \"respiratory_rate\",          # breaths per minute\n",
    "    \"temp_extremities\",          # 1=normal, 2=warm, 3=cool, 4=cold\n",
    "    \"peripheral_pulse\",          # 1=normal, 2=increased, 3=reduced, 4=absent\n",
    "    \"mucous_membranes\",          # 1-6 scale\n",
    "    \"capillary_refill\",          # 1=<3sec, 2=>=3sec\n",
    "    \"pain\",                      # 1=none … 5=extreme\n",
    "    \"peristalsis\",               # 1=hypermotile … 4=absent\n",
    "    \"abdominal_distension\",      # 1=none … 4=severe\n",
    "    \"nasogastric_tube\",          # 1=none, 2=slight, 3=significant\n",
    "    \"nasogastric_reflux\",        # 1=none, 2=>1L, 3=<1L\n",
    "    \"nasogastric_reflux_ph\",\n",
    "    \"rectal_exam_feces\",         # 1=normal … 4=increased\n",
    "    \"abdomen\",                   # 1=normal … 5=distended_large\n",
    "    \"packed_cell_volume\",\n",
    "    \"total_protein\",\n",
    "    \"abdomcentesis_appearance\",  # 1=clear, 2=cloudy, 3=serosanguinous\n",
    "    \"abdomcentesis_total_protein\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_colic_data(filepath):\n",
    "    \"\"\"Load tab-separated horse colic data.\"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filepath) as fh:\n",
    "        for line in fh.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            X.append([float(v) for v in parts[:-1]])\n",
    "            y.append(float(parts[-1]))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_train, y_train = load_colic_data(\"horseColicTraining.txt\")\n",
    "X_test,  y_test  = load_colic_data(\"horseColicTest.txt\")\n",
    "\n",
    "print(f\"Training : {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test     : {X_test.shape[0]} samples\")\n",
    "print(f\"Survival rate — train: {y_train.mean()*100:.1f}%  test: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Correlation matrix — selecting interaction terms\n",
    "\n",
    "The correlation matrix identifies which feature pairs have the strongest linear relationships.  \n",
    "These pairs are the best candidates for interaction terms ($x_i \\times x_j$), which allow the model to capture joint effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(X, names, title=\"Feature Correlation Matrix\"):\n",
    "    \"\"\"Display correlation heatmap.\"\"\"\n",
    "    corr = np.corrcoef(X.T)\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    im = ax.imshow(corr, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_xticks(range(len(names))); ax.set_xticklabels(names, rotation=90, fontsize=7)\n",
    "    ax.set_yticks(range(len(names))); ax.set_yticklabels(names, fontsize=7)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    return corr\n",
    "\n",
    "\n",
    "def top_correlated_pairs(X, names, top_n=5):\n",
    "    \"\"\"Return the top-N most correlated (by |r|) feature pairs.\"\"\"\n",
    "    corr = np.corrcoef(X.T)\n",
    "    pairs = [(i, j, corr[i, j])\n",
    "             for i in range(corr.shape[0])\n",
    "             for j in range(i+1, corr.shape[0])]\n",
    "    pairs.sort(key=lambda t: abs(t[2]), reverse=True)\n",
    "    print(f\"\\nTop {top_n} correlated pairs (interaction term candidates):\")\n",
    "    print(f\"  {'Feature A':<26} {'Feature B':<26} {'r':>8}\")\n",
    "    for i, j, r in pairs[:top_n]:\n",
    "        print(f\"  {names[i]:<26} {names[j]:<26} {r:>8.4f}\")\n",
    "    return pairs[:top_n]\n",
    "\n",
    "\n",
    "corr_matrix = plot_correlation_matrix(X_train, FEATURE_NAMES,\n",
    "                                       \"Feature Correlation Matrix (Training Set)\")\n",
    "top_pairs = top_correlated_pairs(X_train, FEATURE_NAMES, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Add interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_terms(X, pairs):\n",
    "    \"\"\"Append x_i * x_j columns for each (i, j) pair.\"\"\"\n",
    "    cols = [X[:, i] * X[:, j] for i, j, *_ in pairs]\n",
    "    return np.hstack([X] + [c.reshape(-1, 1) for c in cols])\n",
    "\n",
    "\n",
    "# Use top-3 most correlated pairs as interaction terms\n",
    "interaction_pairs = [(i, j) for i, j, _ in top_pairs[:3]]\n",
    "interaction_labels = [(FEATURE_NAMES[i], FEATURE_NAMES[j]) for i, j in interaction_pairs]\n",
    "\n",
    "X_train_aug = add_interaction_terms(X_train, interaction_pairs)\n",
    "X_test_aug  = add_interaction_terms(X_test,  interaction_pairs)\n",
    "\n",
    "print(f\"Interaction terms added: {interaction_labels}\")\n",
    "print(f\"Feature count: {X_train.shape[1]} → {X_train_aug.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature standardization (no data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train_aug)   # fit only on training set\n",
    "X_test_s  = scaler.transform(X_test_aug)        # apply same transform to test\n",
    "\n",
    "print(\"Standardization complete.\")\n",
    "print(f\"Train feature means (first 5): {X_train_s.mean(axis=0)[:5].round(6)}\")\n",
    "print(f\"Train feature stds  (first 5): {X_train_s.std(axis=0)[:5].round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Model A: Custom Stochastic Gradient Ascent\n",
    "\n",
    "Uses a **decaying learning rate** $\\alpha = \\frac{4}{1 + j + i} + 0.01$ to prevent oscillation near convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_ascent(X, y, num_iterations=500, seed=42):\n",
    "    \"\"\"Stochastic gradient ascent with decaying learning rate.\n",
    "\n",
    "    At each iteration, all samples are visited exactly once in random order.\n",
    "    Learning rate alpha = 4/(1+j+i) + 0.01 decreases over time.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    m, n = X.shape\n",
    "    weights = np.ones(n)\n",
    "    loss_history = []\n",
    "\n",
    "    for j in range(num_iterations):\n",
    "        data_index = list(range(m))\n",
    "        for i in range(m):\n",
    "            alpha = 4.0 / (1.0 + j + i) + 0.01      # decaying learning rate\n",
    "            rand_idx = int(random.uniform(0, len(data_index)))\n",
    "            h = sigmoid(np.dot(X[data_index[rand_idx]], weights))\n",
    "            error = y[data_index[rand_idx]] - h\n",
    "            weights += alpha * error * X[data_index[rand_idx]]\n",
    "            del data_index[rand_idx]\n",
    "\n",
    "        # Record log-loss after each complete pass\n",
    "        probs = sigmoid(X @ weights)\n",
    "        loss_history.append(log_loss(y, probs))\n",
    "\n",
    "    return weights, loss_history\n",
    "\n",
    "\n",
    "print(\"Training custom stochastic gradient ascent (500 iterations)...\")\n",
    "weights_sgd, loss_hist_sgd = stochastic_gradient_ascent(X_train_s, y_train, num_iterations=500)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(loss_hist_sgd, color='steelblue', lw=1.5)\n",
    "plt.xlabel('Iteration (full pass)'); plt.ylabel('Log-Loss')\n",
    "plt.title('Convergence — Custom Stochastic Gradient Ascent')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = (sigmoid(X_test_s @ weights_sgd) >= 0.5).astype(float)\n",
    "y_prob_sgd = sigmoid(X_test_s @ weights_sgd)\n",
    "\n",
    "metrics_sgd = evaluate(y_test, y_pred_sgd, y_prob_sgd,\n",
    "                       model_name=\"Custom Stochastic Gradient Ascent\",\n",
    "                       class_names=[\"Did Not Survive\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 — Model B: Sklearn LogisticRegression (L2, lbfgs)\n",
    "\n",
    "Uses the L-BFGS second-order solver with L2 regularization as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=2000, C=1.0, random_state=42)\n",
    "clf.fit(X_train_s, y_train)\n",
    "\n",
    "y_pred_lr = clf.predict(X_test_s)\n",
    "y_prob_lr = clf.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "metrics_lr = evaluate(y_test, y_pred_lr, y_prob_lr,\n",
    "                      model_name=\"Sklearn LogisticRegression (L2, lbfgs)\",\n",
    "                      class_names=[\"Did Not Survive\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ROC curve comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "for name, y_prob in [(\"Custom SGD\", y_prob_sgd), (\"Sklearn LR\", y_prob_lr)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc_val = roc_auc_score(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "plt.plot([0,1],[0,1],'k--', lw=1, label='Random')\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves — Model Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6 — Stratified K-Fold Cross-Validation\n",
    "\n",
    "Provides a robust generalisation estimate by training and evaluating on 5 different data splits.  \n",
    "Standardization is applied **within each fold** to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_kfold_cv(X, y, n_splits=5, C=1.0):\n",
    "    \"\"\"Stratified k-fold CV with per-fold standardization.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        # Fit scaler on fold-train only\n",
    "        sc = StandardScaler()\n",
    "        X_tr_s  = sc.fit_transform(X_tr)\n",
    "        X_val_s = sc.transform(X_val)\n",
    "\n",
    "        model = LogisticRegression(solver=\"lbfgs\", max_iter=2000, C=C, random_state=42)\n",
    "        model.fit(X_tr_s, y_tr)\n",
    "\n",
    "        yp  = model.predict(X_val_s)\n",
    "        ypr = model.predict_proba(X_val_s)[:, 1]\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"accuracy\" : (yp == y_val).mean(),\n",
    "            \"auc_roc\"  : roc_auc_score(y_val, ypr),\n",
    "            \"log_loss\" : log_loss(y_val, ypr),\n",
    "            \"f1\"       : 2*((yp==1)&(y_val==1)).sum() / max((yp==1).sum()+(y_val==1).sum(), 1),\n",
    "            \"mcc\"      : matthews_corrcoef(y_val, yp),\n",
    "        })\n",
    "        print(f\"  Fold {fold}: acc={fold_metrics[-1]['accuracy']:.3f}  \"\n",
    "              f\"auc={fold_metrics[-1]['auc_roc']:.3f}  \"\n",
    "              f\"ll={fold_metrics[-1]['log_loss']:.3f}\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {n_splits}-Fold CV Summary (mean ± std)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for key in fold_metrics[0]:\n",
    "        vals = [m[key] for m in fold_metrics]\n",
    "        print(f\"  {key:<12}: {np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n",
    "    return fold_metrics\n",
    "\n",
    "\n",
    "# Combine train+test for full CV\n",
    "X_full = np.vstack([X_train_aug, X_test_aug])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "print(\"Running 5-fold CV on full dataset (augmented with interaction terms)...\\n\")\n",
    "cv_results = stratified_kfold_cv(X_full, y_full, n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7 — Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"  Final Test-Set Comparison\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {'Metric':<14} {'Custom SGD':>14} {'Sklearn LR':>14}\")\n",
    "print(f\"  {'-'*44}\")\n",
    "for key in [\"accuracy\", \"precision\", \"recall\", \"specificity\",\n",
    "            \"f1\", \"auc_roc\", \"log_loss\", \"mcc\"]:\n",
    "    print(f\"  {key:<14} {metrics_sgd[key]:>14.4f} {metrics_lr[key]:>14.4f}\")\n",
    "\n",
    "print(f\"\\n  Note: error rate = 1 - accuracy\")\n",
    "print(f\"  Custom SGD error rate : {(1-metrics_sgd['accuracy'])*100:.2f}%\")\n",
    "print(f\"  Sklearn LR error rate : {(1-metrics_lr['accuracy'])*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
