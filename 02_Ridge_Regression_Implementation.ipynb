{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Implementation: Complete Workflow\n",
    "\n",
    "**Purpose:** Implement ridge regression with regularization path analysis for model optimization.\n",
    "\n",
    "**Key Concepts:**\n",
    "- L2 Regularization for preventing overfitting\n",
    "- Regularization path visualization\n",
    "- Hyperparameter tuning (Î» selection)\n",
    "- Train/test validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(xMat, yMat, lam=0.2):\n",
    "    \"\"\"\n",
    "    Ridge Regression with L2 Regularization\n",
    "    \n",
    "    Formula: w = (X^T X + Î»I)^-1 X^T y\n",
    "    \n",
    "    Parameters:\n",
    "        xMat: Feature matrix (numpy matrix)\n",
    "        yMat: Target vector (numpy matrix)\n",
    "        lam: Regularization parameter Î»\n",
    "    \n",
    "    Returns:\n",
    "        ws: Regression coefficients\n",
    "    \"\"\"\n",
    "    xTx = xMat.T * xMat\n",
    "    denom = xTx + np.eye(np.shape(xMat)[1]) * lam\n",
    "    \n",
    "    if np.linalg.det(denom) == 0.0:\n",
    "        print(\"âš ï¸  Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = denom.I * (xMat.T * yMat)\n",
    "    return ws\n",
    "\n",
    "\n",
    "def standardize_data(xMat, yMat):\n",
    "    \"\"\"\n",
    "    Standardize features to zero mean and unit variance\n",
    "    CRITICAL: Must be done before ridge regression\n",
    "    \"\"\"\n",
    "    inxMat = xMat.copy()\n",
    "    inyMat = yMat.copy()\n",
    "    \n",
    "    # Standardize y\n",
    "    yMean = np.mean(yMat, 0)\n",
    "    inyMat = yMat - yMean\n",
    "    \n",
    "    # Standardize X\n",
    "    inMeans = np.mean(inxMat, 0)\n",
    "    inVar = np.var(inxMat, 0)\n",
    "    inxMat = (inxMat - inMeans) / inVar\n",
    "    \n",
    "    return inxMat, inyMat\n",
    "\n",
    "\n",
    "def ridge_test_multiple_lambda(xArr, yArr, num_lambda=30):\n",
    "    \"\"\"\n",
    "    Test ridge regression with multiple Î» values\n",
    "    Creates complete regularization path\n",
    "    \n",
    "    Returns:\n",
    "        wMat: Coefficient matrix (each row = one Î» value)\n",
    "        lambda_values: List of Î» values tested\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    \n",
    "    # Standardize data\n",
    "    yMean = np.mean(yMat, axis=0)\n",
    "    yMat = yMat - yMean\n",
    "    xMeans = np.mean(xMat, axis=0)\n",
    "    xVar = np.var(xMat, axis=0)\n",
    "    xMat = (xMat - xMeans) / xVar\n",
    "    \n",
    "    # Test exponentially spaced Î» values\n",
    "    wMat = np.zeros((num_lambda, np.shape(xMat)[1]))\n",
    "    lambda_values = []\n",
    "    \n",
    "    for i in range(num_lambda):\n",
    "        lam = np.exp(i - 10)\n",
    "        lambda_values.append(lam)\n",
    "        ws = ridge_regression(xMat, yMat, lam)\n",
    "        if ws is not None:\n",
    "            wMat[i, :] = ws.T\n",
    "    \n",
    "    return wMat, lambda_values, (xMeans, xVar, yMean)\n",
    "\n",
    "\n",
    "print(\"âœ… Ridge regression functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"Residual Sum of Squares\"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'RSS': rss_error(y_true, y_pred),\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'RÂ²': r2_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_metrics(metrics, label=\"Model\"):\n",
    "    \"\"\"Pretty print metrics\"\"\"\n",
    "    print(f\"\\nðŸ“Š {label} Performance:\")\n",
    "    print(f\"   RSS  = {metrics['RSS']:>10.2f}\")\n",
    "    print(f\"   MSE  = {metrics['MSE']:>10.2f}\")\n",
    "    print(f\"   RMSE = {metrics['RMSE']:>10.2f}\")\n",
    "    print(f\"   RÂ²   = {metrics['RÂ²']:>10.4f}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regularization_path(wMat, lambda_values, feature_names=None):\n",
    "    \"\"\"\n",
    "    Visualize how coefficients change with regularization strength\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot each coefficient\n",
    "    for i in range(wMat.shape[1]):\n",
    "        label = feature_names[i] if feature_names else f'Feature {i+1}'\n",
    "        ax.plot(np.log(lambda_values), wMat[:, i], label=label, linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.set_xlabel('log(Î») - Regularization Strength', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Standardized Coefficients', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Ridge Regression: Regularization Path\\nCoefficient Shrinkage vs. Î»', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_lambda_selection(lambda_values, train_errors, test_errors):\n",
    "    \"\"\"\n",
    "    Plot training and test errors vs. Î» to select optimal regularization\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(np.log(lambda_values), train_errors, 'b-o', label='Training Error', linewidth=2)\n",
    "    ax.plot(np.log(lambda_values), test_errors, 'r-s', label='Test Error', linewidth=2)\n",
    "    \n",
    "    # Find optimal lambda (minimum test error)\n",
    "    optimal_idx = np.argmin(test_errors)\n",
    "    optimal_lambda = lambda_values[optimal_idx]\n",
    "    ax.axvline(x=np.log(optimal_lambda), color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Optimal Î» = {optimal_lambda:.4f}')\n",
    "    \n",
    "    ax.set_xlabel('log(Î»)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Squared Error', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Î» Selection: Training vs. Test Error', fontsize=15, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_lambda\n",
    "\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title=\"Predictions vs. Actual\"):\n",
    "    \"\"\"\n",
    "    Scatter plot of predictions vs. actual values\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    ax.scatter(y_true, y_pred, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Values', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Values', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=15, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"âœ… Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Synthetic Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=200, n_features=10, noise_level=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data with known ground truth\n",
    "    \n",
    "    Parameters:\n",
    "        n_samples: Number of samples\n",
    "        n_features: Number of features\n",
    "        noise_level: Standard deviation of noise\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        true_weights: Ground truth coefficients\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create sparse true weights (only some features are relevant)\n",
    "    true_weights = np.zeros(n_features)\n",
    "    true_weights[0] = 3.0\n",
    "    true_weights[1] = -2.5\n",
    "    true_weights[2] = 1.5\n",
    "    true_weights[4] = -1.0\n",
    "    true_weights[7] = 2.0\n",
    "    # Features 3, 5, 6, 8, 9 are irrelevant (weight = 0)\n",
    "    \n",
    "    # Generate target with noise\n",
    "    y = X @ true_weights + np.random.randn(n_samples) * noise_level\n",
    "    \n",
    "    return X, y, true_weights\n",
    "\n",
    "\n",
    "# Generate data\n",
    "print(\"ðŸ”„ Generating synthetic data...\")\n",
    "X, y, true_weights = generate_synthetic_data(n_samples=200, n_features=10)\n",
    "\n",
    "print(f\"âœ… Data generated:\")\n",
    "print(f\"   Samples: {X.shape[0]}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"\\n   True weights (ground truth):\")\n",
    "print(f\"   {true_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Data split completed:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Train/Test ratio: {X_train.shape[0]/X_test.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ridge Regression: Regularization Path Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RIDGE REGRESSION: REGULARIZATION PATH ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 30 different lambda values\n",
    "print(\"\\nðŸ”„ Testing 30 different Î» values...\")\n",
    "wMat, lambda_values, scaling_params = ridge_test_multiple_lambda(\n",
    "    X_train.tolist(), y_train.tolist(), num_lambda=30\n",
    ")\n",
    "\n",
    "print(f\"âœ… Regularization path computed\")\n",
    "print(f\"   Î» range: {min(lambda_values):.6f} to {max(lambda_values):.2f}\")\n",
    "\n",
    "# Visualize regularization path\n",
    "print(\"\\nðŸ“Š Creating regularization path visualization...\")\n",
    "feature_names = [f'Feature {i+1}' for i in range(X_train.shape[1])]\n",
    "plot_regularization_path(wMat, lambda_values, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimal Î» Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMAL Î» SELECTION USING TRAIN/TEST ERROR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate errors for each lambda on both train and test sets\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "xMeans, xVar, yMean = scaling_params\n",
    "\n",
    "for i, lam in enumerate(lambda_values):\n",
    "    ws = np.mat(wMat[i, :]).T\n",
    "    \n",
    "    # Standardize and predict on training data\n",
    "    X_train_std = (np.mat(X_train) - xMeans) / xVar\n",
    "    y_train_pred = X_train_std * ws\n",
    "    train_error = mean_squared_error(y_train - yMean.A[0], y_train_pred.A)\n",
    "    train_errors.append(train_error)\n",
    "    \n",
    "    # Standardize and predict on test data\n",
    "    X_test_std = (np.mat(X_test) - xMeans) / xVar\n",
    "    y_test_pred = X_test_std * ws\n",
    "    test_error = mean_squared_error(y_test - yMean.A[0], y_test_pred.A)\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "# Plot error curves and find optimal lambda\n",
    "print(\"\\nðŸ“Š Creating Î» selection plot...\")\n",
    "optimal_lambda = plot_lambda_selection(lambda_values, train_errors, test_errors)\n",
    "\n",
    "print(f\"\\nâœ… Optimal Î» selected: {optimal_lambda:.6f}\")\n",
    "optimal_idx = lambda_values.index(optimal_lambda)\n",
    "print(f\"   Training MSE: {train_errors[optimal_idx]:.4f}\")\n",
    "print(f\"   Test MSE: {test_errors[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Training with Optimal Î»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL: RIDGE REGRESSION WITH OPTIMAL Î»\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train final model with optimal lambda\n",
    "print(f\"\\nðŸ”„ Training ridge regression with Î» = {optimal_lambda:.6f}...\")\n",
    "\n",
    "xMat_train = np.mat(X_train)\n",
    "yMat_train = np.mat(y_train).T\n",
    "\n",
    "# Standardize\n",
    "xMat_train_std, yMat_train_std = standardize_data(xMat_train, yMat_train)\n",
    "\n",
    "# Train\n",
    "final_weights = ridge_regression(xMat_train_std, yMat_train_std, optimal_lambda)\n",
    "\n",
    "print(\"\\nâœ… Model trained successfully!\")\n",
    "print(f\"\\nðŸ“Š Learned Coefficients:\")\n",
    "print(final_weights.A.flatten())\n",
    "\n",
    "print(f\"\\nðŸ“Š True Weights (ground truth):\")\n",
    "print(true_weights)\n",
    "\n",
    "# Calculate correlation between learned and true weights\n",
    "correlation = np.corrcoef(final_weights.A.flatten(), true_weights)[0, 1]\n",
    "print(f\"\\nðŸ“ˆ Correlation between learned and true weights: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Predictions on training set\n",
    "y_train_pred = (xMat_train_std * final_weights).A.flatten() + yMean.A[0]\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "print_metrics(train_metrics, \"Training Set\")\n",
    "\n",
    "# Predictions on test set\n",
    "xMat_test = np.mat(X_test)\n",
    "xMat_test_std = (xMat_test - xMeans) / xVar\n",
    "y_test_pred = (xMat_test_std * final_weights).A.flatten() + yMean.A[0]\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "print_metrics(test_metrics, \"Test Set\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nðŸ“Š Overfitting Analysis:\")\n",
    "r2_diff = train_metrics['RÂ²'] - test_metrics['RÂ²']\n",
    "if r2_diff < 0.05:\n",
    "    print(\"   âœ… No significant overfitting detected\")\n",
    "elif r2_diff < 0.15:\n",
    "    print(\"   âš ï¸  Mild overfitting detected\")\n",
    "else:\n",
    "    print(\"   âŒ Significant overfitting detected\")\n",
    "print(f\"   RÂ² difference (train - test): {r2_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization: Predictions vs. Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Creating prediction visualization...\")\n",
    "\n",
    "# Training set\n",
    "plot_predictions(y_train, y_train_pred, \n",
    "                title=f\"Training Set: Predictions vs. Actual\\nRÂ² = {train_metrics['RÂ²']:.4f}\")\n",
    "\n",
    "# Test set\n",
    "plot_predictions(y_test, y_test_pred, \n",
    "                title=f\"Test Set: Predictions vs. Actual\\nRÂ² = {test_metrics['RÂ²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison: Ridge vs. Standard Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: RIDGE vs. STANDARD LINEAR REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standard linear regression (no regularization)\n",
    "xTx = xMat_train_std.T * xMat_train_std\n",
    "if np.linalg.det(xTx) != 0.0:\n",
    "    ws_standard = xTx.I * (xMat_train_std.T * yMat_train_std)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred_std = (xMat_train_std * ws_standard).A.flatten() + yMean.A[0]\n",
    "    y_test_pred_std = (xMat_test_std * ws_standard).A.flatten() + yMean.A[0]\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics_std = calculate_metrics(y_train, y_train_pred_std)\n",
    "    test_metrics_std = calculate_metrics(y_test, y_test_pred_std)\n",
    "    \n",
    "    # Comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        'Method': ['Standard Linear Regression', 'Ridge Regression (Î»={:.6f})'.format(optimal_lambda)],\n",
    "        'Train MSE': [train_metrics_std['MSE'], train_metrics['MSE']],\n",
    "        'Test MSE': [test_metrics_std['MSE'], test_metrics['MSE']],\n",
    "        'Train RÂ²': [train_metrics_std['RÂ²'], train_metrics['RÂ²']],\n",
    "        'Test RÂ²': [test_metrics_std['RÂ²'], test_metrics['RÂ²']]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nðŸ“Š Comparison Results:\")\n",
    "    print(comparison.to_string(index=False))\n",
    "    \n",
    "    # Determine winner\n",
    "    if test_metrics['MSE'] < test_metrics_std['MSE']:\n",
    "        print(\"\\nâœ… Ridge regression performs better (lower test MSE)\")\n",
    "        improvement = (test_metrics_std['MSE'] - test_metrics['MSE']) / test_metrics_std['MSE'] * 100\n",
    "        print(f\"   Improvement: {improvement:.2f}% reduction in test MSE\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Standard regression performs better on this data\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Cannot compute standard regression (singular matrix)\")\n",
    "    print(\"   This is why ridge regression is needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Ridge Regression Implementation Complete!\")\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "print(f\"   1. Optimal Î»: {optimal_lambda:.6f}\")\n",
    "print(f\"   2. Test RÂ²: {test_metrics['RÂ²']:.4f}\")\n",
    "print(f\"   3. Test RMSE: {test_metrics['RMSE']:.4f}\")\n",
    "print(f\"   4. Coefficient-truth correlation: {correlation:.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ“ What We Learned:\")\n",
    "print(\"   â€¢ Ridge regression shrinks coefficients to prevent overfitting\")\n",
    "print(\"   â€¢ Optimal Î» balances bias-variance tradeoff\")\n",
    "print(\"   â€¢ Standardization is CRITICAL before applying regularization\")\n",
    "print(\"   â€¢ Regularization path shows how coefficients change with Î»\")\n",
    "print(\"   â€¢ Cross-validation helps select best Î» value\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"   1. Try with your own dataset\")\n",
    "print(\"   2. Implement k-fold cross-validation\")\n",
    "print(\"   3. Compare with Lasso and Elastic Net\")\n",
    "print(\"   4. Add interaction terms and polynomial features\")\n",
    "print(\"   5. Explore Locally Weighted Linear Regression (LWLR)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Function for Your Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ridge_regression(X_train, y_train, X_test, y_test, lambda_range=None):\n",
    "    \"\"\"\n",
    "    Complete ridge regression workflow for your own data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        lambda_range: Optional list of lambda values to test\n",
    "    \n",
    "    Returns:\n",
    "        optimal_lambda: Best regularization parameter\n",
    "        final_weights: Trained coefficients\n",
    "        test_predictions: Predictions on test set\n",
    "        metrics: Performance metrics\n",
    "    \"\"\"\n",
    "    # Convert to lists if needed\n",
    "    if not isinstance(X_train, list):\n",
    "        X_train = X_train.tolist()\n",
    "    if not isinstance(y_train, list):\n",
    "        y_train = y_train.tolist()\n",
    "    \n",
    "    # Get regularization path\n",
    "    wMat, lambda_values, scaling_params = ridge_test_multiple_lambda(\n",
    "        X_train, y_train, num_lambda=30\n",
    "    )\n",
    "    \n",
    "    # Find optimal lambda using test set\n",
    "    xMeans, xVar, yMean = scaling_params\n",
    "    test_errors = []\n",
    "    \n",
    "    for i, lam in enumerate(lambda_values):\n",
    "        ws = np.mat(wMat[i, :]).T\n",
    "        X_test_std = (np.mat(X_test) - xMeans) / xVar\n",
    "        y_test_pred = X_test_std * ws\n",
    "        test_error = mean_squared_error(y_test - yMean.A[0], y_test_pred.A)\n",
    "        test_errors.append(test_error)\n",
    "    \n",
    "    optimal_idx = np.argmin(test_errors)\n",
    "    optimal_lambda = lambda_values[optimal_idx]\n",
    "    final_weights = np.mat(wMat[optimal_idx, :]).T\n",
    "    \n",
    "    # Make final predictions\n",
    "    X_test_std = (np.mat(X_test) - xMeans) / xVar\n",
    "    test_predictions = (X_test_std * final_weights).A.flatten() + yMean.A[0]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_test, test_predictions)\n",
    "    \n",
    "    return optimal_lambda, final_weights, test_predictions, metrics\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Helper function 'apply_ridge_regression' ready for your data!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  optimal_lambda, weights, predictions, metrics = apply_ridge_regression(\")\n",
    "print(\"      X_train, y_train, X_test, y_test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ready for Your Own Data!\n",
    "\n",
    "To use this with your own dataset:\n",
    "\n",
    "```python\n",
    "# Load your data\n",
    "X = your_features  # numpy array or pandas DataFrame\n",
    "y = your_target    # numpy array or pandas Series\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Apply ridge regression\n",
    "optimal_lambda, weights, predictions, metrics = apply_ridge_regression(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# View results\n",
    "print_metrics(metrics, \"Your Model\")\n",
    "```\n",
    "\n",
    "**Repository:** https://github.com/enzodata3-blip/Task4  \n",
    "**Based on:** https://github.com/Jack-Cherish/Machine-Learning  \n",
    "**Created:** 2026-02-09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
