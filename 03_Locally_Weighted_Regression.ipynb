{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Weighted Linear Regression (LWLR)\n",
    "\n",
    "**Purpose:** Implement non-parametric regression using Gaussian kernel weighting.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Non-parametric regression (no fixed model form)\n",
    "- Gaussian kernel weighting\n",
    "- Bandwidth selection (k parameter)\n",
    "- Bias-variance tradeoff\n",
    "\n",
    "**When to use LWLR:**\n",
    "- Non-linear relationships\n",
    "- Local patterns in data\n",
    "- When global linear model fails\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core LWLR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lwlr(testPoint, xArr, yArr, k=1.0):\n",
    "    \"\"\"\n",
    "    Locally Weighted Linear Regression for single point\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Calculate distance from testPoint to all training points\n",
    "        2. Assign weights using Gaussian kernel: w = exp(-distance¬≤/2k¬≤)\n",
    "        3. Fit weighted linear regression\n",
    "        4. Return prediction\n",
    "    \n",
    "    Parameters:\n",
    "        testPoint: Point to predict (1D array)\n",
    "        xArr: Training features\n",
    "        yArr: Training targets\n",
    "        k: Bandwidth parameter\n",
    "           - Small k (e.g., 0.01): Very local fit, high variance\n",
    "           - Medium k (e.g., 1.0): Balanced\n",
    "           - Large k (e.g., 10): Global fit, high bias\n",
    "    \n",
    "    Returns:\n",
    "        Predicted value for testPoint\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    m = np.shape(xMat)[0]\n",
    "    \n",
    "    # Initialize diagonal weight matrix\n",
    "    weights = np.mat(np.eye((m)))\n",
    "    \n",
    "    # Calculate weight for each training point\n",
    "    for j in range(m):\n",
    "        diffMat = testPoint - xMat[j, :]\n",
    "        # Gaussian kernel: weight decreases with distance\n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T / (-2.0 * k**2))\n",
    "    \n",
    "    # Weighted least squares: w = (X^T W X)^-1 X^T W y\n",
    "    xTx = xMat.T * (weights * xMat)\n",
    "    \n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"‚ö†Ô∏è  Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = xTx.I * (xMat.T * (weights * yMat))\n",
    "    return testPoint * ws\n",
    "\n",
    "\n",
    "def lwlr_test(testArr, xArr, yArr, k=1.0):\n",
    "    \"\"\"\n",
    "    Apply LWLR to multiple test points\n",
    "    \n",
    "    Note: Can be slow for large datasets since it fits\n",
    "          a new model for EACH test point\n",
    "    \"\"\"\n",
    "    m = np.shape(testArr)[0]\n",
    "    yHat = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        yHat[i] = lwlr(testArr[i], xArr, yArr, k)\n",
    "    \n",
    "    return yHat\n",
    "\n",
    "\n",
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"Residual Sum of Squares\"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()\n",
    "\n",
    "\n",
    "print(\"‚úÖ LWLR functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Non-Linear Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_data(n_samples=200, noise=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate non-linear data where LWLR should outperform linear regression\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate X from 0 to 10\n",
    "    x = np.linspace(0, 10, n_samples)\n",
    "    \n",
    "    # Non-linear function: combination of sine and quadratic\n",
    "    y = 2 * np.sin(x) + 0.5 * x**2 - 3 * x + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Add intercept term\n",
    "    X = np.column_stack([np.ones(n_samples), x])\n",
    "    \n",
    "    return X, y, x\n",
    "\n",
    "\n",
    "# Generate data\n",
    "print(\"üîÑ Generating non-linear data...\")\n",
    "X, y, x_raw = generate_nonlinear_data(n_samples=200, noise=0.5)\n",
    "\n",
    "print(f\"‚úÖ Data generated: {X.shape[0]} samples\")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_raw, y, alpha=0.5, s=30)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Non-Linear Data: LWLR vs. Linear Regression', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Also split raw x for visualization\n",
    "x_train_raw = X_train[:, 1]\n",
    "x_test_raw = X_test[:, 1]\n",
    "\n",
    "print(f\"‚úÖ Data split:\")\n",
    "print(f\"   Training: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bandwidth (k) Selection Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BANDWIDTH (k) SELECTION EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test multiple k values\n",
    "k_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "print(\"\\nüîÑ Testing different bandwidth values...\\n\")\n",
    "print(f\"{'k Value':<12} | {'Train RSS':>12} | {'Test RSS':>12} | {'Test R¬≤':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Predictions on training set\n",
    "    yHat_train = lwlr_test(X_train, X_train, y_train, k)\n",
    "    rss_train = rss_error(y_train, yHat_train)\n",
    "    \n",
    "    # Predictions on test set\n",
    "    yHat_test = lwlr_test(X_test, X_train, y_train, k)\n",
    "    rss_test = rss_error(y_test, yHat_test)\n",
    "    r2_test = r2_score(y_test, yHat_test)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'train_rss': rss_train,\n",
    "        'test_rss': rss_test,\n",
    "        'test_r2': r2_test,\n",
    "        'predictions': yHat_test\n",
    "    })\n",
    "    \n",
    "    print(f\"{k:<12.2f} | {rss_train:>12.2f} | {rss_test:>12.2f} | {r2_test:>10.4f}\")\n",
    "\n",
    "# Find optimal k\n",
    "optimal_result = min(results, key=lambda x: x['test_rss'])\n",
    "optimal_k = optimal_result['k']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ Optimal k: {optimal_k:.2f} (lowest test RSS)\")\n",
    "print(f\"   Test RSS: {optimal_result['test_rss']:.2f}\")\n",
    "print(f\"   Test R¬≤: {optimal_result['test_r2']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Effect of Bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating bandwidth comparison visualization...\")\n",
    "\n",
    "# Create grid for smooth predictions\n",
    "x_grid = np.linspace(x_raw.min(), x_raw.max(), 200)\n",
    "X_grid = np.column_stack([np.ones(200), x_grid])\n",
    "\n",
    "# Plot predictions for different k values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "k_to_plot = [0.01, 0.1, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "for idx, k in enumerate(k_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions\n",
    "    result = [r for r in results if r['k'] == k][0]\n",
    "    \n",
    "    # Predict on grid for smooth curve\n",
    "    y_grid = lwlr_test(X_grid, X_train, y_train, k)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_train_raw, y_train, alpha=0.3, s=20, label='Training data')\n",
    "    ax.scatter(x_test_raw, y_test, alpha=0.3, s=20, color='red', label='Test data')\n",
    "    ax.plot(x_grid, y_grid, 'g-', linewidth=2, label='LWLR fit')\n",
    "    \n",
    "    ax.set_title(f'k = {k:.2f}\\nTest R¬≤ = {result[\"test_r2\"]:.3f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('X', fontsize=10)\n",
    "    ax.set_ylabel('y', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight optimal\n",
    "    if k == optimal_k:\n",
    "        ax.set_facecolor('#ffe6e6')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('red')\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ Small k (0.01): Overfitting - very wiggly, high variance\")\n",
    "print(\"   ‚Ä¢ Medium k (1.0-2.0): Balanced - captures non-linearity, generalizes well\")\n",
    "print(\"   ‚Ä¢ Large k (10.0): Underfitting - too smooth, approaches linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: LWLR vs. Standard Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: LWLR vs. STANDARD LINEAR REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standard linear regression\n",
    "xMat_train = np.mat(X_train)\n",
    "yMat_train = np.mat(y_train).T\n",
    "xMat_test = np.mat(X_test)\n",
    "\n",
    "xTx = xMat_train.T * xMat_train\n",
    "ws_standard = xTx.I * (xMat_train.T * yMat_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_std = (xMat_train * ws_standard).A.flatten()\n",
    "y_test_pred_std = (xMat_test * ws_standard).A.flatten()\n",
    "\n",
    "# LWLR with optimal k\n",
    "y_train_pred_lwlr = lwlr_test(X_train, X_train, y_train, optimal_k)\n",
    "y_test_pred_lwlr = lwlr_test(X_test, X_train, y_train, optimal_k)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\nüìä Performance Comparison:\\n\")\n",
    "print(f\"{'Method':<35} | {'Train RSS':>12} | {'Test RSS':>12} | {'Test R¬≤':>10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Standard regression\n",
    "train_rss_std = rss_error(y_train, y_train_pred_std)\n",
    "test_rss_std = rss_error(y_test, y_test_pred_std)\n",
    "test_r2_std = r2_score(y_test, y_test_pred_std)\n",
    "print(f\"{'Standard Linear Regression':<35} | {train_rss_std:>12.2f} | {test_rss_std:>12.2f} | {test_r2_std:>10.4f}\")\n",
    "\n",
    "# LWLR\n",
    "train_rss_lwlr = rss_error(y_train, y_train_pred_lwlr)\n",
    "test_rss_lwlr = rss_error(y_test, y_test_pred_lwlr)\n",
    "test_r2_lwlr = r2_score(y_test, y_test_pred_lwlr)\n",
    "print(f\"{'LWLR (k=' + f'{optimal_k:.2f}' + ')':<35} | {train_rss_lwlr:>12.2f} | {test_rss_lwlr:>12.2f} | {test_r2_lwlr:>10.4f}\")\n",
    "\n",
    "# Improvement\n",
    "improvement = (test_rss_std - test_rss_lwlr) / test_rss_std * 100\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(f\"‚úÖ LWLR improves test RSS by {improvement:.2f}%\")\n",
    "print(f\"‚úÖ LWLR improves test R¬≤ from {test_r2_std:.4f} to {test_r2_lwlr:.4f}\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Side-by-Side Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating side-by-side comparison...\")\n",
    "\n",
    "# Create smooth prediction grid\n",
    "x_grid = np.linspace(x_raw.min(), x_raw.max(), 300)\n",
    "X_grid = np.column_stack([np.ones(300), x_grid])\n",
    "\n",
    "# Standard regression predictions on grid\n",
    "xMat_grid = np.mat(X_grid)\n",
    "y_grid_std = (xMat_grid * ws_standard).A.flatten()\n",
    "\n",
    "# LWLR predictions on grid\n",
    "y_grid_lwlr = lwlr_test(X_grid, X_train, y_train, optimal_k)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Standard Linear Regression\n",
    "ax1.scatter(x_train_raw, y_train, alpha=0.4, s=30, label='Training data', color='blue')\n",
    "ax1.scatter(x_test_raw, y_test, alpha=0.4, s=30, label='Test data', color='red')\n",
    "ax1.plot(x_grid, y_grid_std, 'g-', linewidth=3, label='Linear fit')\n",
    "ax1.set_xlabel('X', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Standard Linear Regression\\nTest R¬≤ = {test_r2_std:.4f}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# LWLR\n",
    "ax2.scatter(x_train_raw, y_train, alpha=0.4, s=30, label='Training data', color='blue')\n",
    "ax2.scatter(x_test_raw, y_test, alpha=0.4, s=30, label='Test data', color='red')\n",
    "ax2.plot(x_grid, y_grid_lwlr, 'purple', linewidth=3, label=f'LWLR fit (k={optimal_k:.2f})')\n",
    "ax2.set_xlabel('X', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Locally Weighted Linear Regression\\nTest R¬≤ = {test_r2_lwlr:.4f}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")\n",
    "print(\"\\nüéì Key Observation:\")\n",
    "print(\"   LWLR captures the non-linear pattern, while standard regression\")\n",
    "print(\"   is constrained to a straight line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bias-Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BIAS-VARIANCE TRADEOFF ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot train/test error vs. k\n",
    "k_values_plot = [r['k'] for r in results]\n",
    "train_errors = [r['train_rss'] for r in results]\n",
    "test_errors = [r['test_rss'] for r in results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(k_values_plot, train_errors, 'b-o', linewidth=2, markersize=8, label='Training RSS')\n",
    "ax.plot(k_values_plot, test_errors, 'r-s', linewidth=2, markersize=8, label='Test RSS')\n",
    "\n",
    "# Mark optimal k\n",
    "ax.axvline(x=optimal_k, color='green', linestyle='--', linewidth=2, \n",
    "          label=f'Optimal k = {optimal_k:.2f}')\n",
    "\n",
    "ax.set_xlabel('Bandwidth (k)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('RSS Error', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Bias-Variance Tradeoff: Training vs. Test Error', \n",
    "            fontsize=15, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('High Variance\\n(Overfitting)', xy=(0.01, train_errors[0]), \n",
    "           xytext=(0.02, train_errors[0]*1.5),\n",
    "           fontsize=10, ha='center',\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3),\n",
    "           arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "\n",
    "ax.annotate('High Bias\\n(Underfitting)', xy=(10, test_errors[-1]), \n",
    "           xytext=(5, test_errors[-1]*1.2),\n",
    "           fontsize=10, ha='center',\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3),\n",
    "           arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéì Interpretation:\")\n",
    "print(\"   ‚Ä¢ Small k: Model is too flexible ‚Üí high variance ‚Üí overfitting\")\n",
    "print(\"   ‚Ä¢ Large k: Model is too simple ‚Üí high bias ‚Üí underfitting\")\n",
    "print(f\"   ‚Ä¢ Optimal k = {optimal_k:.2f}: Best balance between bias and variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: LOCALLY WEIGHTED LINEAR REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Key Findings:\")\n",
    "print(f\"   ‚Ä¢ Optimal bandwidth: k = {optimal_k:.2f}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤ (LWLR): {test_r2_lwlr:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤ (Linear): {test_r2_std:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {improvement:.2f}% reduction in test error\")\n",
    "\n",
    "print(\"\\nüéì When to Use LWLR:\")\n",
    "print(\"   ‚úì Data has non-linear relationships\")\n",
    "print(\"   ‚úì Local patterns are important\")\n",
    "print(\"   ‚úì You have moderate dataset size (not huge)\")\n",
    "print(\"   ‚úì Interpretability is not the main concern\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  When NOT to Use LWLR:\")\n",
    "print(\"   ‚úó Very large datasets (slow prediction)\")\n",
    "print(\"   ‚úó High-dimensional data (curse of dimensionality)\")\n",
    "print(\"   ‚úó Need fast real-time predictions\")\n",
    "print(\"   ‚úó Need interpretable coefficients\")\n",
    "\n",
    "print(\"\\nüí° Advantages:\")\n",
    "print(\"   + No assumptions about functional form\")\n",
    "print(\"   + Captures non-linear patterns automatically\")\n",
    "print(\"   + Single hyperparameter (k) to tune\")\n",
    "print(\"   + Straightforward implementation\")\n",
    "\n",
    "print(\"\\n‚ö° Disadvantages:\")\n",
    "print(\"   - Computationally expensive (O(n) per prediction)\")\n",
    "print(\"   - Must store all training data\")\n",
    "print(\"   - Suffers from curse of dimensionality\")\n",
    "print(\"   - Bandwidth selection can be tricky\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Try LWLR on your own data\")\n",
    "print(\"   2. Implement cross-validation for k selection\")\n",
    "print(\"   3. Compare with polynomial regression\")\n",
    "print(\"   4. Explore other kernel functions (Epanechnikov, Tricube)\")\n",
    "print(\"   5. Combine with feature engineering\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Function for Your Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lwlr(X_train, y_train, X_test, y_test, k_values=None):\n",
    "    \"\"\"\n",
    "    Complete LWLR workflow for your own data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        k_values: Optional list of k values to test\n",
    "    \n",
    "    Returns:\n",
    "        optimal_k: Best bandwidth parameter\n",
    "        predictions: Predictions on test set\n",
    "        metrics: Performance metrics\n",
    "    \"\"\"\n",
    "    if k_values is None:\n",
    "        k_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    \n",
    "    best_rss = float('inf')\n",
    "    optimal_k = None\n",
    "    best_predictions = None\n",
    "    \n",
    "    for k in k_values:\n",
    "        y_pred = lwlr_test(X_test, X_train, y_train, k)\n",
    "        rss = rss_error(y_test, y_pred)\n",
    "        \n",
    "        if rss < best_rss:\n",
    "            best_rss = rss\n",
    "            optimal_k = k\n",
    "            best_predictions = y_pred\n",
    "    \n",
    "    metrics = {\n",
    "        'RSS': best_rss,\n",
    "        'MSE': mean_squared_error(y_test, best_predictions),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, best_predictions)),\n",
    "        'R¬≤': r2_score(y_test, best_predictions)\n",
    "    }\n",
    "    \n",
    "    return optimal_k, best_predictions, metrics\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Helper function 'apply_lwlr' ready for your data!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  optimal_k, predictions, metrics = apply_lwlr(X_train, y_train, X_test, y_test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ready for Your Own Data!\n",
    "\n",
    "To use this with your own dataset:\n",
    "\n",
    "```python\n",
    "# Load your data\n",
    "X = your_features\n",
    "y = your_target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Apply LWLR\n",
    "optimal_k, predictions, metrics = apply_lwlr(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"Optimal k: {optimal_k}\")\n",
    "print(f\"Test R¬≤: {metrics['R¬≤']:.4f}\")\n",
    "```\n",
    "\n",
    "**Repository:** https://github.com/enzodata3-blip/Task4  \n",
    "**Based on:** https://github.com/Jack-Cherish/Machine-Learning  \n",
    "**Created:** 2026-02-09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
