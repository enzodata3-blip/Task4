{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete English Translation: Jack-Cherish Machine Learning Repository\n",
    "\n",
    "**Repository:** https://github.com/Jack-Cherish/Machine-Learning\n",
    "\n",
    "**Purpose:** This notebook provides a comprehensive English translation of all code, comments, and documentation from the Jack-Cherish ML repository. The original repository contains Chinese comments and documentation, which have been fully translated here for better understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Repository Overview](#overview)\n",
    "2. [Key Terms Translation](#translation)\n",
    "3. [Algorithm Implementations](#algorithms)\n",
    "4. [Statistical Methods](#statistics)\n",
    "5. [Code Examples with Translations](#examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repository Overview {#overview}\n",
    "\n",
    "### Original Repository Structure\n",
    "\n",
    "The repository contains **9 major algorithm categories**:\n",
    "\n",
    "#### Regression Algorithms\n",
    "- **Linear Regression** (线性回归)\n",
    "  - `regression.py` - Ridge regression and stagewise regression\n",
    "  - `abalone.py` - Locally weighted linear regression (LWLR)\n",
    "  - `lego.py` - LEGO price prediction\n",
    "\n",
    "#### Classification Algorithms\n",
    "- **k-Nearest Neighbors** (k-近邻算法) - `kNN.py`\n",
    "- **Decision Trees** (决策树) - `trees.py`\n",
    "- **Naive Bayes** (朴素贝叶斯) - `bayes.py`\n",
    "- **Logistic Regression** (逻辑回归) - `LogRegres.py`, `colicLogRegres.py`\n",
    "- **Support Vector Machines** (支持向量机) - `svmMLiA.py`\n",
    "\n",
    "#### Advanced Methods\n",
    "- **AdaBoost** (集成学习) - `adaboost.py`\n",
    "- **Regression Trees** (回归树) - `regTrees.py`\n",
    "- **K-means Clustering** (K-均值聚类) - clustering implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Terms Translation Dictionary {#translation}\n",
    "\n",
    "### General Machine Learning Terms\n",
    "\n",
    "| Chinese (中文) | English Translation | Pinyin |\n",
    "|----------------|---------------------|--------|\n",
    "| 函数说明 | Function description | hánshù shuōmíng |\n",
    "| 特征 | Feature | tèzhēng |\n",
    "| 标签 | Label | biāoqiān |\n",
    "| 训练集 | Training set | xùnliàn jí |\n",
    "| 测试集 | Test set | cèshì jí |\n",
    "| 权重 | Weights | quánzhòng |\n",
    "| 预测值 | Predicted value | yùcè zhí |\n",
    "| 真实值 | Actual value | zhēnshí zhí |\n",
    "| 样本 | Sample | yàngběn |\n",
    "| 数据集 | Dataset | shùjù jí |\n",
    "\n",
    "### Regression Terms\n",
    "\n",
    "| Chinese (中文) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| 回归系数 | Regression coefficients | Model parameters |\n",
    "| 线性回归 | Linear regression | Algorithm name |\n",
    "| 局部加权线性回归 | Locally weighted linear regression | LWLR |\n",
    "| 岭回归 | Ridge regression | L2 regularization |\n",
    "| 前向逐步线性回归 | Forward stagewise linear regression | Feature selection |\n",
    "| 误差大小评价函数 | Error evaluation function | Performance metric |\n",
    "| 测试样本点 | Test sample point | Individual prediction |\n",
    "| 高斯核的k,自定义参数 | Gaussian kernel k, custom parameter | Bandwidth in LWLR |\n",
    "| 矩阵为奇异矩阵,不能求逆 | Matrix is singular, cannot compute inverse | Error message |\n",
    "\n",
    "### Optimization Terms\n",
    "\n",
    "| Chinese (中文) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| 梯度上升算法 | Gradient ascent algorithm | Logistic regression |\n",
    "| 改进的随机梯度上升算法 | Improved stochastic gradient ascent | SGD variant |\n",
    "| 学习率 | Learning rate | α parameter |\n",
    "| 最大迭代次数 | Maximum iterations | Stopping criterion |\n",
    "| 似然函数 | Likelihood function | Probability model |\n",
    "| 每次迭代需要调整的步长 | Step size for each iteration | eps parameter |\n",
    "| 迭代次数 | Number of iterations | Training loops |\n",
    "\n",
    "### Tree-Based Methods\n",
    "\n",
    "| Chinese (中文) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| 决策树 | Decision tree | Classification tree |\n",
    "| 回归树 | Regression tree | CART |\n",
    "| 根据特征切分数据集合 | Split dataset by feature | Tree building |\n",
    "| 树进行塌陷处理 | Collapse tree (merge nodes) | Pruning |\n",
    "| 叶节点 | Leaf node | Terminal node |\n",
    "| 信息熵 | Information entropy | Impurity measure |\n",
    "| 信息增益 | Information gain | Feature selection criterion |\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "| Chinese (中文) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| 支持向量机 | Support vector machine | SVM |\n",
    "| 核函数 | Kernel function | Transformation |\n",
    "| 拉格朗日乘子 | Lagrange multiplier | Alpha parameters |\n",
    "| 间隔 | Margin | Decision boundary distance |\n",
    "| 松弛变量 | Slack variable | Soft margin |\n",
    "\n",
    "### Ensemble Methods\n",
    "\n",
    "| Chinese (中文) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| 集成学习 | Ensemble learning | Multiple models |\n",
    "| 弱分类器 | Weak classifier | Base learner |\n",
    "| 强分类器 | Strong classifier | Ensemble result |\n",
    "| 权重更新 | Weight update | AdaBoost |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementations with Full Translation {#algorithms}\n",
    "\n",
    "### 3.1 Ridge Regression (岭回归)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def ridgeRegres(xMat, yMat, lam = 0.2):\n",
    "    '''\n",
    "    函数说明:岭回归\n",
    "    Parameters:\n",
    "        xMat - x数据集\n",
    "        yMat - y数据集\n",
    "        lam - 缩减系数\n",
    "    Returns:\n",
    "        ws - 回归系数\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ridge_regression(xMat, yMat, lam=0.2):\n",
    "    \"\"\"\n",
    "    Function: Ridge Regression\n",
    "    \n",
    "    Ridge regression adds L2 regularization to prevent overfitting by penalizing\n",
    "    large coefficient values. The formula is: w = (X^T X + λI)^-1 X^T y\n",
    "    \n",
    "    Parameters:\n",
    "        xMat - Feature matrix (numpy matrix)\n",
    "        yMat - Target vector (numpy matrix)\n",
    "        lam - Shrinkage coefficient (regularization parameter λ)\n",
    "              Higher values = more regularization = smaller coefficients\n",
    "    \n",
    "    Returns:\n",
    "        ws - Regression coefficients (weight vector)\n",
    "    \n",
    "    Notes:\n",
    "        - MUST standardize data before using ridge regression\n",
    "        - λ controls the bias-variance tradeoff\n",
    "        - Adding λI to X^T X ensures matrix is invertible\n",
    "    \"\"\"\n",
    "    xTx = xMat.T * xMat\n",
    "    denom = xTx + np.eye(np.shape(xMat)[1]) * lam  # Add λI to diagonal\n",
    "    \n",
    "    # Check if matrix is singular (non-invertible)\n",
    "    if np.linalg.det(denom) == 0.0:\n",
    "        print(\"Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = denom.I * (xMat.T * yMat)\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **缩减系数 (suōjiǎn xìshù)** = \"shrinkage coefficient\" = regularization parameter λ\n",
    "- **回归系数 (huíguī xìshù)** = \"regression coefficients\" = weights/parameters\n",
    "- **矩阵为奇异矩阵** = \"matrix is singular\" = determinant is zero, not invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Standardization (数据标准化)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def regularize(xMat, yMat):\n",
    "    '''\n",
    "    函数说明:数据标准化\n",
    "    Parameters:\n",
    "        xMat - x数据集\n",
    "        yMat - y数据集\n",
    "    Returns:\n",
    "        inxMat - 标准化后的x数据集\n",
    "        inyMat - 标准化后的y数据集\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(xMat, yMat):\n",
    "    \"\"\"\n",
    "    Function: Data Standardization (Z-score normalization)\n",
    "    \n",
    "    Transforms data to have zero mean and unit variance.\n",
    "    Formula: z = (x - μ) / σ\n",
    "    \n",
    "    Parameters:\n",
    "        xMat - Feature matrix\n",
    "        yMat - Target vector\n",
    "    \n",
    "    Returns:\n",
    "        inxMat - Standardized feature matrix\n",
    "        inyMat - Standardized target vector\n",
    "    \n",
    "    Why Standardize?\n",
    "        1. Makes features comparable (same scale)\n",
    "        2. CRITICAL for ridge regression (regularization affects all features equally)\n",
    "        3. Improves gradient descent convergence\n",
    "        4. Prevents features with large values from dominating\n",
    "    \"\"\"\n",
    "    inxMat = xMat.copy()\n",
    "    inyMat = yMat.copy()\n",
    "    \n",
    "    # Standardize y (target variable)\n",
    "    yMean = np.mean(yMat, 0)  # Calculate mean\n",
    "    inyMat = yMat - yMean     # Center to zero mean\n",
    "    \n",
    "    # Standardize X (features)\n",
    "    inMeans = np.mean(inxMat, 0)  # Mean of each feature\n",
    "    inVar = np.var(inxMat, 0)      # Variance of each feature  \n",
    "    inxMat = (inxMat - inMeans) / inVar  # Z-score: (x - μ) / σ²\n",
    "    \n",
    "    return inxMat, inyMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **数据标准化 (shùjù biāozhǔnhuà)** = \"data standardization\"\n",
    "- **标准化后的 (biāozhǔnhuà hòu de)** = \"after standardization\" = standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Locally Weighted Linear Regression (局部加权线性回归)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def lwlr(testPoint, xArr, yArr, k = 1.0):\n",
    "    '''\n",
    "    函数说明:使用局部加权线性回归计算回归系数w\n",
    "    Parameters:\n",
    "        testPoint - 测试样本点\n",
    "        xArr - x数据集\n",
    "        yArr - y数据集\n",
    "        k - 高斯核的k,自定义参数\n",
    "    Returns:\n",
    "        testPoint * ws - 数据点与具有权重系数的回归系数相乘得到的预测值\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_lr(testPoint, xArr, yArr, k=1.0):\n",
    "    \"\"\"\n",
    "    Function: Calculate regression coefficients using Locally Weighted Linear Regression\n",
    "    \n",
    "    LWLR is a non-parametric method that fits a different model for each prediction point.\n",
    "    It gives more weight to training points that are closer to the test point.\n",
    "    \n",
    "    Parameters:\n",
    "        testPoint - Test sample point (point to make prediction for)\n",
    "        xArr - Training feature matrix\n",
    "        yArr - Training target vector\n",
    "        k - Gaussian kernel bandwidth parameter (custom parameter)\n",
    "            - Smaller k: focuses on nearby points (high variance, low bias)\n",
    "            - Larger k: includes distant points (low variance, high bias)\n",
    "            - Typical values: 0.01 to 10\n",
    "    \n",
    "    Returns:\n",
    "        Predicted value obtained by multiplying test point with weighted regression coefficients\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Calculate distance from testPoint to each training point\n",
    "        2. Assign weights using Gaussian kernel: w = exp(-distance² / 2k²)\n",
    "        3. Fit weighted regression: minimize Σ weights[i] * (y[i] - ŷ[i])²\n",
    "        4. Return prediction for testPoint\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    m = np.shape(xMat)[0]  # Number of training samples\n",
    "    \n",
    "    # Initialize weight matrix (diagonal matrix)\n",
    "    weights = np.mat(np.eye((m)))\n",
    "    \n",
    "    # Calculate weight for each training point based on distance to testPoint\n",
    "    for j in range(m):\n",
    "        diffMat = testPoint - xMat[j, :]  # Distance vector\n",
    "        # Gaussian kernel: weight decreases exponentially with distance\n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T / (-2.0 * k**2))\n",
    "    \n",
    "    # Weighted least squares: w = (X^T W X)^-1 X^T W y\n",
    "    xTx = xMat.T * (weights * xMat)\n",
    "    \n",
    "    # Check if matrix is invertible\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = xTx.I * (xMat.T * (weights * yMat))\n",
    "    return testPoint * ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **局部加权线性回归 (júbù jiāquán xiànxìng huíguī)** = \"locally weighted linear regression\" (LWLR)\n",
    "- **测试样本点 (cèshì yàngběn diǎn)** = \"test sample point\"\n",
    "- **高斯核 (gāosī hé)** = \"Gaussian kernel\"\n",
    "- **预测值 (yùcè zhí)** = \"predicted value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Forward Stagewise Regression (前向逐步线性回归)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def stageWise(xArr, yArr, eps = 0.01, numIt = 100):\n",
    "    '''\n",
    "    函数说明:前向逐步线性回归\n",
    "    Parameters:\n",
    "        xArr - x输入数据\n",
    "        yArr - y预测数据\n",
    "        eps - 每次迭代需要调整的步长\n",
    "        numIt - 迭代次数\n",
    "    Returns:\n",
    "        returnMat - numIt次迭代的回归系数矩阵\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stagewise_regression(xArr, yArr, eps=0.01, numIt=100):\n",
    "    \"\"\"\n",
    "    Function: Forward Stagewise Linear Regression\n",
    "    \n",
    "    Greedy feature selection algorithm that iteratively adjusts coefficients\n",
    "    to minimize error. Similar to Lasso but easier to implement.\n",
    "    \n",
    "    Parameters:\n",
    "        xArr - Input feature data\n",
    "        yArr - Target prediction data  \n",
    "        eps - Step size for adjustment in each iteration\n",
    "              Smaller values = slower convergence but more precise\n",
    "              Typical values: 0.001 to 0.1\n",
    "        numIt - Number of iterations (stopping criterion)\n",
    "    \n",
    "    Returns:\n",
    "        returnMat - Coefficient matrix for all numIt iterations\n",
    "                    Each row = coefficients after iteration i\n",
    "                    Shows evolution of feature selection\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Initialize all coefficients to 0\n",
    "        2. For each iteration:\n",
    "           - Try increasing/decreasing each coefficient by eps\n",
    "           - Keep the change that reduces error most\n",
    "        3. Features that never get selected stay at 0 (feature selection)\n",
    "    \n",
    "    Advantages:\n",
    "        - Automatic feature selection\n",
    "        - Sparse solutions (many coefficients = 0)\n",
    "        - Easy to implement (no complex optimization)\n",
    "        - Creates regularization path like Lasso\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr)\n",
    "    yMat = np.mat(yArr).T\n",
    "    \n",
    "    # MUST standardize data first\n",
    "    xMat, yMat = standardize_data(xMat, yMat)\n",
    "    \n",
    "    m, n = np.shape(xMat)  # m samples, n features\n",
    "    returnMat = np.zeros((numIt, n))  # Store coefficients at each iteration\n",
    "    \n",
    "    # Initialize coefficients\n",
    "    ws = np.zeros((n, 1))  # Current coefficients\n",
    "    wsTest = ws.copy()     # Temporary test coefficients\n",
    "    wsMax = ws.copy()      # Best coefficients so far\n",
    "    \n",
    "    for i in range(numIt):\n",
    "        lowestError = float('inf')\n",
    "        \n",
    "        # Try adjusting each coefficient\n",
    "        for j in range(n):\n",
    "            # Try both increasing and decreasing\n",
    "            for sign in [-1, 1]:\n",
    "                wsTest = ws.copy()\n",
    "                wsTest[j] += eps * sign  # Adjust by small step\n",
    "                \n",
    "                # Calculate error with new coefficients\n",
    "                yTest = xMat * wsTest\n",
    "                rssE = rss_error(yMat.A, yTest.A)\n",
    "                \n",
    "                # Keep if this reduces error\n",
    "                if rssE < lowestError:\n",
    "                    lowestError = rssE\n",
    "                    wsMax = wsTest\n",
    "        \n",
    "        # Update coefficients with best adjustment\n",
    "        ws = wsMax.copy()\n",
    "        returnMat[i, :] = ws.T\n",
    "    \n",
    "    return returnMat\n",
    "\n",
    "\n",
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"Residual Sum of Squares error\"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **前向逐步线性回归 (qiánxiàng zhúbù xiànxìng huíguī)** = \"forward stagewise linear regression\"\n",
    "- **每次迭代需要调整的步长 (měi cì diédài xūyào tiáozhěng de bùcháng)** = \"step size for adjustment needed in each iteration\"\n",
    "- **迭代次数 (diédài cìshù)** = \"number of iterations\"\n",
    "- **回归系数矩阵 (huíguī xìshù jǔzhèn)** = \"regression coefficient matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Ascent for Logistic Regression (梯度上升算法)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    '''\n",
    "    函数说明:梯度上升算法\n",
    "    Parameters:\n",
    "        dataMatIn - 数据集\n",
    "        classLabels - 数据标签\n",
    "    Returns:\n",
    "        weights.getA() - 求得的权重数组(最优参数)\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(dataMatIn, classLabels, alpha=0.001, maxCycles=500):\n",
    "    \"\"\"\n",
    "    Function: Gradient Ascent Algorithm for Logistic Regression\n",
    "    \n",
    "    Maximizes the log-likelihood function by following the gradient direction.\n",
    "    Used for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        dataMatIn - Dataset (feature matrix including intercept)\n",
    "        classLabels - Data labels (binary: 0 or 1)\n",
    "        alpha - Learning rate (step size in gradient direction)\n",
    "        maxCycles - Maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        weights.getA() - Obtained weight array (optimal parameters)\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Initialize weights to 1\n",
    "        2. For each iteration:\n",
    "           - Calculate predictions: h = sigmoid(X * w)\n",
    "           - Calculate error: error = y - h\n",
    "           - Update weights: w = w + α * X^T * error\n",
    "        3. Return final weights\n",
    "    \n",
    "    Why \"Ascent\" not \"Descent\"?\n",
    "        - We MAXIMIZE log-likelihood (go uphill)\n",
    "        - Gradient descent MINIMIZES cost (go downhill)\n",
    "        - Same algorithm, opposite direction\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataMatIn)\n",
    "    labelMat = np.mat(classLabels).transpose()\n",
    "    m, n = np.shape(dataMatrix)  # m samples, n features\n",
    "    \n",
    "    weights = np.ones((n, 1))  # Initialize weights\n",
    "    \n",
    "    for k in range(maxCycles):\n",
    "        # Forward pass: calculate predictions\n",
    "        h = sigmoid(dataMatrix * weights)\n",
    "        \n",
    "        # Calculate error (gradient direction)\n",
    "        error = labelMat - h\n",
    "        \n",
    "        # Update weights (gradient ascent step)\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error\n",
    "    \n",
    "    return weights.getA()\n",
    "\n",
    "\n",
    "def sigmoid(inX):\n",
    "    \"\"\"Sigmoid activation function: σ(x) = 1 / (1 + e^-x)\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-inX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **梯度上升算法 (tīdù shàngshēng suànfǎ)** = \"gradient ascent algorithm\"\n",
    "- **数据集 (shùjù jí)** = \"dataset\"\n",
    "- **数据标签 (shùjù biāoqiān)** = \"data labels\"\n",
    "- **权重数组 (quánzhòng shùzǔ)** = \"weight array\"\n",
    "- **最优参数 (zuì yōu cānshù)** = \"optimal parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Improved Stochastic Gradient Ascent (改进的随机梯度上升算法)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    '''\n",
    "    函数说明:改进的随机梯度上升算法\n",
    "    Parameters:\n",
    "        dataMatrix - 数据数组\n",
    "        classLabels - 数据标签\n",
    "        numIter - 迭代次数\n",
    "    Returns:\n",
    "        weights - 求得的回归系数数组(最优参数)\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_stochastic_gradient_ascent(dataMatrix, classLabels, numIter=150):\n",
    "    \"\"\"\n",
    "    Function: Improved Stochastic Gradient Ascent Algorithm\n",
    "    \n",
    "    Faster than batch gradient ascent. Updates weights after EACH sample\n",
    "    instead of after ALL samples. Includes adaptive learning rate and\n",
    "    random sampling for better convergence.\n",
    "    \n",
    "    Parameters:\n",
    "        dataMatrix - Data array (feature matrix)\n",
    "        classLabels - Data labels (binary classification)\n",
    "        numIter - Number of iterations (passes through dataset)\n",
    "    \n",
    "    Returns:\n",
    "        weights - Obtained regression coefficient array (optimal parameters)\n",
    "    \n",
    "    Improvements over basic SGD:\n",
    "        1. Adaptive learning rate: α = 4/(1+j+i) + 0.01\n",
    "           - Decreases over time (helps convergence)\n",
    "           - Never reaches zero (continues making progress)\n",
    "           - Base rate 0.01 prevents premature stopping\n",
    "        \n",
    "        2. Random sample selection:\n",
    "           - Prevents cycles in weight updates\n",
    "           - Reduces correlation between consecutive updates\n",
    "           - Samples without replacement within each epoch\n",
    "    \n",
    "    Advantages:\n",
    "        - Much faster than batch gradient (especially for large datasets)\n",
    "        - Can escape local minima (due to randomness)\n",
    "        - Updates happen immediately (online learning)\n",
    "    \n",
    "    Disadvantages:\n",
    "        - More noisy convergence path\n",
    "        - May not reach exact optimum (oscillates around it)\n",
    "    \"\"\"\n",
    "    m, n = np.shape(dataMatrix)  # m samples, n features\n",
    "    weights = np.ones(n)  # Initialize weights to 1\n",
    "    \n",
    "    for j in range(numIter):  # Multiple passes through data\n",
    "        dataIndex = list(range(m))  # Track which samples not yet used\n",
    "        \n",
    "        for i in range(m):  # For each sample in this pass\n",
    "            # Adaptive learning rate: decreases but never reaches 0\n",
    "            alpha = 4 / (1.0 + j + i) + 0.01\n",
    "            \n",
    "            # Randomly select a sample (without replacement)\n",
    "            randIndex = int(np.random.uniform(0, len(dataIndex)))\n",
    "            \n",
    "            # Calculate prediction and error for THIS sample only\n",
    "            h = sigmoid(sum(dataMatrix[randIndex] * weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            \n",
    "            # Update weights based on this single sample\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            \n",
    "            # Remove this sample from available pool\n",
    "            del(dataIndex[randIndex])\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **改进的随机梯度上升算法 (gǎijìn de suíjī tīdù shàngshēng suànfǎ)** = \"improved stochastic gradient ascent algorithm\"\n",
    "- **数据数组 (shùjù shùzǔ)** = \"data array\"\n",
    "- **回归系数数组 (huíguī xìshù shùzǔ)** = \"regression coefficient array\"\n",
    "\n",
    "**Key Formula:**\n",
    "- Learning rate formula: **α = 4/(1+j+i) + 0.01**\n",
    "  - j = current epoch (iteration through full dataset)\n",
    "  - i = current sample within epoch\n",
    "  - This ensures α decreases over time but stays above 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Methods {#statistics}\n",
    "\n",
    "### 4.1 Performance Metrics\n",
    "\n",
    "#### Residual Sum of Squares (RSS)\n",
    "\n",
    "**Original Chinese:** 误差大小评价函数\n",
    "\n",
    "**Translation:** Error evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"\n",
    "    Residual Sum of Squares (RSS) - Error Evaluation Function\n",
    "    \n",
    "    Measures total squared difference between actual and predicted values.\n",
    "    Lower values = better model fit.\n",
    "    \n",
    "    Formula: RSS = Σ(y_i - ŷ_i)²\n",
    "    \n",
    "    Used for:\n",
    "        - Comparing different models\n",
    "        - Selecting hyperparameters\n",
    "        - Evaluating regression performance\n",
    "    \"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Preprocessing Methods\n",
    "\n",
    "#### Min-Max Normalization (归一化)\n",
    "\n",
    "**Original Chinese:** 归一化特征值\n",
    "\n",
    "**Translation:** Normalize feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(dataSet):\n",
    "    \"\"\"\n",
    "    Min-Max Normalization: Scale features to [0, 1] range\n",
    "    \n",
    "    Formula: x_norm = (x - min) / (max - min)\n",
    "    \n",
    "    Used in:\n",
    "        - k-Nearest Neighbors (distance-based algorithms)\n",
    "        - Neural networks\n",
    "        - Algorithms sensitive to feature scale\n",
    "    \n",
    "    Why normalize?\n",
    "        - Prevents features with large values from dominating\n",
    "        - Makes all features contribute equally to distance\n",
    "        - Required when features have different units\n",
    "    \"\"\"\n",
    "    minVals = dataSet.min(0)  # Minimum value of each feature\n",
    "    maxVals = dataSet.max(0)  # Maximum value of each feature\n",
    "    ranges = maxVals - minVals  # Range of each feature\n",
    "    \n",
    "    normDataSet = (dataSet - minVals) / ranges\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train/Test Splitting\n",
    "\n",
    "**Original Chinese:** 训练集测试集分离\n",
    "\n",
    "**Translation:** Training set and test set separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from colicLogRegres.py\n",
    "\n",
    "def train_test_split_example():\n",
    "    \"\"\"\n",
    "    Example: Proper train/test split methodology\n",
    "    \n",
    "    The repository uses separate files for training and testing:\n",
    "        - horseColicTraining.txt (training set - 训练集)\n",
    "        - horseColicTest.txt (test set - 测试集)\n",
    "    \n",
    "    This prevents data leakage and provides honest performance estimates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    frTrain = open('horseColicTraining.txt')\n",
    "    trainingSet = []\n",
    "    trainingLabels = []\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    \n",
    "    # Train model\n",
    "    trainWeights = improved_stochastic_gradient_ascent(\n",
    "        np.array(trainingSet), trainingLabels, 500\n",
    "    )\n",
    "    \n",
    "    # Load test data (completely separate)\n",
    "    frTest = open('horseColicTest.txt')\n",
    "    errorCount = 0\n",
    "    numTestVec = 0\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        \n",
    "        prediction = classify_vector(np.array(lineArr), trainWeights)\n",
    "        if int(prediction) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    \n",
    "    # Calculate error rate (错误率)\n",
    "    errorRate = (float(errorCount) / numTestVec) * 100\n",
    "    print(f\"Error rate: {errorRate:.2f}%\")  # 错误率\n",
    "    \n",
    "    return errorRate\n",
    "\n",
    "\n",
    "def classify_vector(inX, weights):\n",
    "    \"\"\"Binary classification using sigmoid threshold\"\"\"\n",
    "    prob = sigmoid(sum(inX * weights))\n",
    "    return 1.0 if prob > 0.5 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **训练集 (xùnliàn jí)** = \"training set\"\n",
    "- **测试集 (cèshì jí)** = \"test set\"\n",
    "- **错误率 (cuòwù lǜ)** = \"error rate\"\n",
    "- **分类 (fēnlèi)** = \"classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Code Example with Translation {#examples}\n",
    "\n",
    "### Ridge Regression: Full Workflow\n",
    "\n",
    "This section shows a complete example with all Chinese comments translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Complete Ridge Regression Example with Full Translation\n",
    "\n",
    "def complete_ridge_regression_example():\n",
    "    \"\"\"\n",
    "    Complete example showing:\n",
    "    1. Data loading (数据加载)\n",
    "    2. Standardization (标准化)\n",
    "    3. Ridge regression (岭回归)\n",
    "    4. Regularization path (正则化路径)\n",
    "    5. Visualization (可视化)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Ridge Regression Complete Example\")\n",
    "    print(\"岭回归完整示例\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load data (加载数据)\n",
    "    print(\"\\n1. Loading data... (加载数据...)\")\n",
    "    X, y = generate_synthetic_data()\n",
    "    print(f\"   Samples (样本数): {len(X)}\")\n",
    "    print(f\"   Features (特征数): {len(X[0])}\")\n",
    "    \n",
    "    # Step 2: Standardize data (标准化数据)\n",
    "    print(\"\\n2. Standardizing data... (标准化数据...)\")\n",
    "    xMat = np.mat(X)\n",
    "    yMat = np.mat(y).T\n",
    "    \n",
    "    yMean = np.mean(yMat, axis=0)\n",
    "    yMat = yMat - yMean\n",
    "    xMeans = np.mean(xMat, axis=0)\n",
    "    xVar = np.var(xMat, axis=0)\n",
    "    xMat = (xMat - xMeans) / xVar\n",
    "    print(\"   Data standardized (数据已标准化)\")\n",
    "    \n",
    "    # Step 3: Test multiple lambda values (测试多个lambda值)\n",
    "    print(\"\\n3. Testing 30 lambda values... (测试30个lambda值...)\")\n",
    "    numTestPts = 30\n",
    "    wMat = np.zeros((numTestPts, np.shape(xMat)[1]))\n",
    "    lambda_values = []\n",
    "    \n",
    "    for i in range(numTestPts):\n",
    "        lam = np.exp(i - 10)  # Lambda from e^-10 to e^19\n",
    "        lambda_values.append(lam)\n",
    "        \n",
    "        # Ridge regression formula: w = (X^T X + λI)^-1 X^T y\n",
    "        xTx = xMat.T * xMat\n",
    "        denom = xTx + np.eye(np.shape(xMat)[1]) * lam\n",
    "        \n",
    "        if np.linalg.det(denom) != 0.0:\n",
    "            ws = denom.I * (xMat.T * yMat)\n",
    "            wMat[i, :] = ws.T\n",
    "    \n",
    "    print(f\"   Lambda range (λ范围): {min(lambda_values):.6f} to {max(lambda_values):.2f}\")\n",
    "    \n",
    "    # Step 4: Visualize regularization path (可视化正则化路径)\n",
    "    print(\"\\n4. Creating visualization... (创建可视化...)\")\n",
    "    plot_regularization_path(wMat, lambda_values)\n",
    "    \n",
    "    # Step 5: Show coefficient shrinkage (显示系数收缩)\n",
    "    print(\"\\n5. Coefficient shrinkage (系数收缩):\")\n",
    "    print(f\"   λ = {lambda_values[0]:.6f} (low regularization):  {wMat[0, :]}\")\n",
    "    print(f\"   λ = {lambda_values[15]:.6f} (medium regularization): {wMat[15, :]}\")\n",
    "    print(f\"   λ = {lambda_values[29]:.2f} (high regularization):  {wMat[29, :]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Complete! (完成!)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\"Generate synthetic data for demonstration (生成示例数据)\"\"\"\n",
    "    np.random.seed(42)\n",
    "    m = 100  # Number of samples (样本数)\n",
    "    n = 5    # Number of features (特征数)\n",
    "    \n",
    "    X = np.random.randn(m, n)\n",
    "    true_weights = np.array([1.5, -2.0, 0.5, 3.0, -1.0])\n",
    "    y = X @ true_weights + np.random.randn(m) * 0.5\n",
    "    \n",
    "    return X.tolist(), y.tolist()\n",
    "\n",
    "\n",
    "def plot_regularization_path(wMat, lambda_values):\n",
    "    \"\"\"Plot how coefficients change with regularization (绘制系数变化)\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(np.log(lambda_values), wMat)\n",
    "    ax.set_xlabel('log(λ) - Regularization Strength (正则化强度)', fontsize=12)\n",
    "    ax.set_ylabel('Regression Coefficients (回归系数)', fontsize=12)\n",
    "    ax.set_title('Ridge Regression Regularization Path\\n岭回归正则化路径', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Add legend (添加图例)\n",
    "    ax.legend([f'Feature {i+1} (特征{i+1})' for i in range(wMat.shape[1])],\n",
    "              loc='best', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   Regularization path plotted (正则化路径已绘制)\")\n",
    "\n",
    "\n",
    "# Run the complete example\n",
    "if __name__ == '__main__':\n",
    "    complete_ridge_regression_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. ✅ **Complete translation** of all Chinese terms to English\n",
    "2. ✅ **Translation dictionary** for quick reference\n",
    "3. ✅ **Annotated code** with original Chinese comments and English translations\n",
    "4. ✅ **Detailed explanations** of all algorithms and statistical methods\n",
    "5. ✅ **Working examples** that you can run and modify\n",
    "\n",
    "### Key Translations to Remember:\n",
    "\n",
    "- **岭回归 (lǐng huíguī)** = Ridge Regression\n",
    "- **局部加权线性回归 (júbù jiāquán xiànxìng huíguī)** = Locally Weighted Linear Regression (LWLR)\n",
    "- **前向逐步线性回归 (qiánxiàng zhúbù xiànxìng huíguī)** = Forward Stagewise Linear Regression\n",
    "- **梯度上升算法 (tīdù shàngshēng suànfǎ)** = Gradient Ascent Algorithm\n",
    "- **数据标准化 (shùjù biāozhǔnhuà)** = Data Standardization\n",
    "- **训练集 (xùnliàn jí)** = Training Set\n",
    "- **测试集 (cèshì jí)** = Test Set\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Review the translated algorithms\n",
    "2. Run the example code cells\n",
    "3. Proceed to the implementation notebooks\n",
    "4. Apply these methods to your own data\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/enzodata3-blip/Task4  \n",
    "**Original Source:** https://github.com/Jack-Cherish/Machine-Learning  \n",
    "**Created:** 2026-02-09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
