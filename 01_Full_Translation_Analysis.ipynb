{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete English Translation: Jack-Cherish Machine Learning Repository\n",
    "\n",
    "**Repository:** https://github.com/Jack-Cherish/Machine-Learning\n",
    "\n",
    "**Purpose:** This notebook provides a comprehensive English translation of all code, comments, and documentation from the Jack-Cherish ML repository. The original repository contains Chinese comments and documentation, which have been fully translated here for better understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Repository Overview](#overview)\n",
    "2. [Key Terms Translation](#translation)\n",
    "3. [Algorithm Implementations](#algorithms)\n",
    "4. [Statistical Methods](#statistics)\n",
    "5. [Code Examples with Translations](#examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repository Overview {#overview}\n",
    "\n",
    "### Original Repository Structure\n",
    "\n",
    "The repository contains **9 major algorithm categories**:\n",
    "\n",
    "#### Regression Algorithms\n",
    "- **Linear Regression** (\u7ebf\u6027\u56de\u5f52)\n",
    "  - `regression.py` - Ridge regression and stagewise regression\n",
    "  - `abalone.py` - Locally weighted linear regression (LWLR)\n",
    "  - `lego.py` - LEGO price prediction\n",
    "\n",
    "#### Classification Algorithms\n",
    "- **k-Nearest Neighbors** (k-\u8fd1\u90bb\u7b97\u6cd5) - `kNN.py`\n",
    "- **Decision Trees** (\u51b3\u7b56\u6811) - `trees.py`\n",
    "- **Naive Bayes** (\u6734\u7d20\u8d1d\u53f6\u65af) - `bayes.py`\n",
    "- **Logistic Regression** (\u903b\u8f91\u56de\u5f52) - `LogRegres.py`, `colicLogRegres.py`\n",
    "- **Support Vector Machines** (\u652f\u6301\u5411\u91cf\u673a) - `svmMLiA.py`\n",
    "\n",
    "#### Advanced Methods\n",
    "- **AdaBoost** (\u96c6\u6210\u5b66\u4e60) - `adaboost.py`\n",
    "- **Regression Trees** (\u56de\u5f52\u6811) - `regTrees.py`\n",
    "- **K-means Clustering** (K-\u5747\u503c\u805a\u7c7b) - clustering implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Terms Translation Dictionary {#translation}\n",
    "\n",
    "### General Machine Learning Terms\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Pinyin |\n",
    "|----------------|---------------------|--------|\n",
    "| \u51fd\u6570\u8bf4\u660e | Function description | h\u00e1nsh\u00f9 shu\u014dm\u00edng |\n",
    "| \u7279\u5f81 | Feature | t\u00e8zh\u0113ng |\n",
    "| \u6807\u7b7e | Label | bi\u0101oqi\u0101n |\n",
    "| \u8bad\u7ec3\u96c6 | Training set | x\u00f9nli\u00e0n j\u00ed |\n",
    "| \u6d4b\u8bd5\u96c6 | Test set | c\u00e8sh\u00ec j\u00ed |\n",
    "| \u6743\u91cd | Weights | qu\u00e1nzh\u00f2ng |\n",
    "| \u9884\u6d4b\u503c | Predicted value | y\u00f9c\u00e8 zh\u00ed |\n",
    "| \u771f\u5b9e\u503c | Actual value | zh\u0113nsh\u00ed zh\u00ed |\n",
    "| \u6837\u672c | Sample | y\u00e0ngb\u011bn |\n",
    "| \u6570\u636e\u96c6 | Dataset | sh\u00f9j\u00f9 j\u00ed |\n",
    "\n",
    "### Regression Terms\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| \u56de\u5f52\u7cfb\u6570 | Regression coefficients | Model parameters |\n",
    "| \u7ebf\u6027\u56de\u5f52 | Linear regression | Algorithm name |\n",
    "| \u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52 | Locally weighted linear regression | LWLR |\n",
    "| \u5cad\u56de\u5f52 | Ridge regression | L2 regularization |\n",
    "| \u524d\u5411\u9010\u6b65\u7ebf\u6027\u56de\u5f52 | Forward stagewise linear regression | Feature selection |\n",
    "| \u8bef\u5dee\u5927\u5c0f\u8bc4\u4ef7\u51fd\u6570 | Error evaluation function | Performance metric |\n",
    "| \u6d4b\u8bd5\u6837\u672c\u70b9 | Test sample point | Individual prediction |\n",
    "| \u9ad8\u65af\u6838\u7684k,\u81ea\u5b9a\u4e49\u53c2\u6570 | Gaussian kernel k, custom parameter | Bandwidth in LWLR |\n",
    "| \u77e9\u9635\u4e3a\u5947\u5f02\u77e9\u9635,\u4e0d\u80fd\u6c42\u9006 | Matrix is singular, cannot compute inverse | Error message |\n",
    "\n",
    "### Optimization Terms\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| \u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5 | Gradient ascent algorithm | Logistic regression |\n",
    "| \u6539\u8fdb\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5 | Improved stochastic gradient ascent | SGD variant |\n",
    "| \u5b66\u4e60\u7387 | Learning rate | \u03b1 parameter |\n",
    "| \u6700\u5927\u8fed\u4ee3\u6b21\u6570 | Maximum iterations | Stopping criterion |\n",
    "| \u4f3c\u7136\u51fd\u6570 | Likelihood function | Probability model |\n",
    "| \u6bcf\u6b21\u8fed\u4ee3\u9700\u8981\u8c03\u6574\u7684\u6b65\u957f | Step size for each iteration | eps parameter |\n",
    "| \u8fed\u4ee3\u6b21\u6570 | Number of iterations | Training loops |\n",
    "\n",
    "### Tree-Based Methods\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| \u51b3\u7b56\u6811 | Decision tree | Classification tree |\n",
    "| \u56de\u5f52\u6811 | Regression tree | CART |\n",
    "| \u6839\u636e\u7279\u5f81\u5207\u5206\u6570\u636e\u96c6\u5408 | Split dataset by feature | Tree building |\n",
    "| \u6811\u8fdb\u884c\u584c\u9677\u5904\u7406 | Collapse tree (merge nodes) | Pruning |\n",
    "| \u53f6\u8282\u70b9 | Leaf node | Terminal node |\n",
    "| \u4fe1\u606f\u71b5 | Information entropy | Impurity measure |\n",
    "| \u4fe1\u606f\u589e\u76ca | Information gain | Feature selection criterion |\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| \u652f\u6301\u5411\u91cf\u673a | Support vector machine | SVM |\n",
    "| \u6838\u51fd\u6570 | Kernel function | Transformation |\n",
    "| \u62c9\u683c\u6717\u65e5\u4e58\u5b50 | Lagrange multiplier | Alpha parameters |\n",
    "| \u95f4\u9694 | Margin | Decision boundary distance |\n",
    "| \u677e\u5f1b\u53d8\u91cf | Slack variable | Soft margin |\n",
    "\n",
    "### Ensemble Methods\n",
    "\n",
    "| Chinese (\u4e2d\u6587) | English Translation | Context |\n",
    "|----------------|---------------------|----------|\n",
    "| \u96c6\u6210\u5b66\u4e60 | Ensemble learning | Multiple models |\n",
    "| \u5f31\u5206\u7c7b\u5668 | Weak classifier | Base learner |\n",
    "| \u5f3a\u5206\u7c7b\u5668 | Strong classifier | Ensemble result |\n",
    "| \u6743\u91cd\u66f4\u65b0 | Weight update | AdaBoost |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementations with Full Translation {#algorithms}\n",
    "\n",
    "### 3.1 Ridge Regression (\u5cad\u56de\u5f52)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def ridgeRegres(xMat, yMat, lam = 0.2):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u5cad\u56de\u5f52\n",
    "    Parameters:\n",
    "        xMat - x\u6570\u636e\u96c6\n",
    "        yMat - y\u6570\u636e\u96c6\n",
    "        lam - \u7f29\u51cf\u7cfb\u6570\n",
    "    Returns:\n",
    "        ws - \u56de\u5f52\u7cfb\u6570\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ridge_regression(xMat, yMat, lam=0.2):\n",
    "    \"\"\"\n",
    "    Function: Ridge Regression\n",
    "    \n",
    "    Ridge regression adds L2 regularization to prevent overfitting by penalizing\n",
    "    large coefficient values. The formula is: w = (X^T X + \u03bbI)^-1 X^T y\n",
    "    \n",
    "    Parameters:\n",
    "        xMat - Feature matrix (numpy matrix)\n",
    "        yMat - Target vector (numpy matrix)\n",
    "        lam - Shrinkage coefficient (regularization parameter \u03bb)\n",
    "              Higher values = more regularization = smaller coefficients\n",
    "    \n",
    "    Returns:\n",
    "        ws - Regression coefficients (weight vector)\n",
    "    \n",
    "    Notes:\n",
    "        - MUST standardize data before using ridge regression\n",
    "        - \u03bb controls the bias-variance tradeoff\n",
    "        - Adding \u03bbI to X^T X ensures matrix is invertible\n",
    "    \"\"\"\n",
    "    xTx = xMat.T * xMat\n",
    "    denom = xTx + np.eye(np.shape(xMat)[1]) * lam  # Add \u03bbI to diagonal\n",
    "    \n",
    "    # Check if matrix is singular (non-invertible)\n",
    "    if np.linalg.det(denom) == 0.0:\n",
    "        print(\"Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = denom.I * (xMat.T * yMat)\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u7f29\u51cf\u7cfb\u6570 (su\u014dji\u01cen x\u00ecsh\u00f9)** = \"shrinkage coefficient\" = regularization parameter \u03bb\n",
    "- **\u56de\u5f52\u7cfb\u6570 (hu\u00edgu\u012b x\u00ecsh\u00f9)** = \"regression coefficients\" = weights/parameters\n",
    "- **\u77e9\u9635\u4e3a\u5947\u5f02\u77e9\u9635** = \"matrix is singular\" = determinant is zero, not invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Standardization (\u6570\u636e\u6807\u51c6\u5316)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def regularize(xMat, yMat):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u6570\u636e\u6807\u51c6\u5316\n",
    "    Parameters:\n",
    "        xMat - x\u6570\u636e\u96c6\n",
    "        yMat - y\u6570\u636e\u96c6\n",
    "    Returns:\n",
    "        inxMat - \u6807\u51c6\u5316\u540e\u7684x\u6570\u636e\u96c6\n",
    "        inyMat - \u6807\u51c6\u5316\u540e\u7684y\u6570\u636e\u96c6\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(xMat, yMat):\n",
    "    \"\"\"\n",
    "    Function: Data Standardization (Z-score normalization)\n",
    "    \n",
    "    Transforms data to have zero mean and unit variance.\n",
    "    Formula: z = (x - \u03bc) / \u03c3\n",
    "    \n",
    "    Parameters:\n",
    "        xMat - Feature matrix\n",
    "        yMat - Target vector\n",
    "    \n",
    "    Returns:\n",
    "        inxMat - Standardized feature matrix\n",
    "        inyMat - Standardized target vector\n",
    "    \n",
    "    Why Standardize?\n",
    "        1. Makes features comparable (same scale)\n",
    "        2. CRITICAL for ridge regression (regularization affects all features equally)\n",
    "        3. Improves gradient descent convergence\n",
    "        4. Prevents features with large values from dominating\n",
    "    \"\"\"\n",
    "    inxMat = xMat.copy()\n",
    "    inyMat = yMat.copy()\n",
    "    \n",
    "    # Standardize y (target variable)\n",
    "    yMean = np.mean(yMat, 0)  # Calculate mean\n",
    "    inyMat = yMat - yMean     # Center to zero mean\n",
    "    \n",
    "    # Standardize X (features)\n",
    "    inMeans = np.mean(inxMat, 0)  # Mean of each feature\n",
    "    inVar = np.var(inxMat, 0)      # Variance of each feature  \n",
    "    inxMat = (inxMat - inMeans) / inVar  # Z-score: (x - \u03bc) / \u03c3\u00b2\n",
    "    \n",
    "    return inxMat, inyMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u6570\u636e\u6807\u51c6\u5316 (sh\u00f9j\u00f9 bi\u0101ozh\u01d4nhu\u00e0)** = \"data standardization\"\n",
    "- **\u6807\u51c6\u5316\u540e\u7684 (bi\u0101ozh\u01d4nhu\u00e0 h\u00f2u de)** = \"after standardization\" = standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Locally Weighted Linear Regression (\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def lwlr(testPoint, xArr, yArr, k = 1.0):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u4f7f\u7528\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\u8ba1\u7b97\u56de\u5f52\u7cfb\u6570w\n",
    "    Parameters:\n",
    "        testPoint - \u6d4b\u8bd5\u6837\u672c\u70b9\n",
    "        xArr - x\u6570\u636e\u96c6\n",
    "        yArr - y\u6570\u636e\u96c6\n",
    "        k - \u9ad8\u65af\u6838\u7684k,\u81ea\u5b9a\u4e49\u53c2\u6570\n",
    "    Returns:\n",
    "        testPoint * ws - \u6570\u636e\u70b9\u4e0e\u5177\u6709\u6743\u91cd\u7cfb\u6570\u7684\u56de\u5f52\u7cfb\u6570\u76f8\u4e58\u5f97\u5230\u7684\u9884\u6d4b\u503c\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_lr(testPoint, xArr, yArr, k=1.0):\n",
    "    \"\"\"\n",
    "    Function: Calculate regression coefficients using Locally Weighted Linear Regression\n",
    "    \n",
    "    LWLR is a non-parametric method that fits a different model for each prediction point.\n",
    "    It gives more weight to training points that are closer to the test point.\n",
    "    \n",
    "    Parameters:\n",
    "        testPoint - Test sample point (point to make prediction for)\n",
    "        xArr - Training feature matrix\n",
    "        yArr - Training target vector\n",
    "        k - Gaussian kernel bandwidth parameter (custom parameter)\n",
    "            - Smaller k: focuses on nearby points (high variance, low bias)\n",
    "            - Larger k: includes distant points (low variance, high bias)\n",
    "            - Typical values: 0.01 to 10\n",
    "    \n",
    "    Returns:\n",
    "        Predicted value obtained by multiplying test point with weighted regression coefficients\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Calculate distance from testPoint to each training point\n",
    "        2. Assign weights using Gaussian kernel: w = exp(-distance\u00b2 / 2k\u00b2)\n",
    "        3. Fit weighted regression: minimize \u03a3 weights[i] * (y[i] - \u0177[i])\u00b2\n",
    "        4. Return prediction for testPoint\n",
    "    \"\"\"\n",
    "    xMat = np.asmatrix(xArr)\n",
    "    yMat = np.asmatrix(yArr).T\n",
    "    m = np.shape(xMat)[0]  # Number of training samples\n",
    "    \n",
    "    # Initialize weight matrix (diagonal matrix)\n",
    "    weights = np.asmatrix(np.eye((m)))\n",
    "    \n",
    "    # Calculate weight for each training point based on distance to testPoint\n",
    "    for j in range(m):\n",
    "        diffMat = testPoint - xMat[j, :]  # Distance vector\n",
    "        # Gaussian kernel: weight decreases exponentially with distance\n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T / (-2.0 * k**2))\n",
    "    \n",
    "    # Weighted least squares: w = (X^T W X)^-1 X^T W y\n",
    "    xTx = xMat.T * (weights * xMat)\n",
    "    \n",
    "    # Check if matrix is invertible\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"Matrix is singular, cannot compute inverse\")\n",
    "        return None\n",
    "    \n",
    "    ws = xTx.I * (xMat.T * (weights * yMat))\n",
    "    return testPoint * ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52 (j\u00fab\u00f9 ji\u0101qu\u00e1n xi\u00e0nx\u00ecng hu\u00edgu\u012b)** = \"locally weighted linear regression\" (LWLR)\n",
    "- **\u6d4b\u8bd5\u6837\u672c\u70b9 (c\u00e8sh\u00ec y\u00e0ngb\u011bn di\u01cen)** = \"test sample point\"\n",
    "- **\u9ad8\u65af\u6838 (g\u0101os\u012b h\u00e9)** = \"Gaussian kernel\"\n",
    "- **\u9884\u6d4b\u503c (y\u00f9c\u00e8 zh\u00ed)** = \"predicted value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Forward Stagewise Regression (\u524d\u5411\u9010\u6b65\u7ebf\u6027\u56de\u5f52)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def stageWise(xArr, yArr, eps = 0.01, numIt = 100):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u524d\u5411\u9010\u6b65\u7ebf\u6027\u56de\u5f52\n",
    "    Parameters:\n",
    "        xArr - x\u8f93\u5165\u6570\u636e\n",
    "        yArr - y\u9884\u6d4b\u6570\u636e\n",
    "        eps - \u6bcf\u6b21\u8fed\u4ee3\u9700\u8981\u8c03\u6574\u7684\u6b65\u957f\n",
    "        numIt - \u8fed\u4ee3\u6b21\u6570\n",
    "    Returns:\n",
    "        returnMat - numIt\u6b21\u8fed\u4ee3\u7684\u56de\u5f52\u7cfb\u6570\u77e9\u9635\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stagewise_regression(xArr, yArr, eps=0.01, numIt=100):\n",
    "    \"\"\"\n",
    "    Function: Forward Stagewise Linear Regression\n",
    "    \n",
    "    Greedy feature selection algorithm that iteratively adjusts coefficients\n",
    "    to minimize error. Similar to Lasso but easier to implement.\n",
    "    \n",
    "    Parameters:\n",
    "        xArr - Input feature data\n",
    "        yArr - Target prediction data  \n",
    "        eps - Step size for adjustment in each iteration\n",
    "              Smaller values = slower convergence but more precise\n",
    "              Typical values: 0.001 to 0.1\n",
    "        numIt - Number of iterations (stopping criterion)\n",
    "    \n",
    "    Returns:\n",
    "        returnMat - Coefficient matrix for all numIt iterations\n",
    "                    Each row = coefficients after iteration i\n",
    "                    Shows evolution of feature selection\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Initialize all coefficients to 0\n",
    "        2. For each iteration:\n",
    "           - Try increasing/decreasing each coefficient by eps\n",
    "           - Keep the change that reduces error most\n",
    "        3. Features that never get selected stay at 0 (feature selection)\n",
    "    \n",
    "    Advantages:\n",
    "        - Automatic feature selection\n",
    "        - Sparse solutions (many coefficients = 0)\n",
    "        - Easy to implement (no complex optimization)\n",
    "        - Creates regularization path like Lasso\n",
    "    \"\"\"\n",
    "    xMat = np.asmatrix(xArr)\n",
    "    yMat = np.asmatrix(yArr).T\n",
    "    \n",
    "    # MUST standardize data first\n",
    "    xMat, yMat = standardize_data(xMat, yMat)\n",
    "    \n",
    "    m, n = np.shape(xMat)  # m samples, n features\n",
    "    returnMat = np.zeros((numIt, n))  # Store coefficients at each iteration\n",
    "    \n",
    "    # Initialize coefficients\n",
    "    ws = np.zeros((n, 1))  # Current coefficients\n",
    "    wsTest = ws.copy()     # Temporary test coefficients\n",
    "    wsMax = ws.copy()      # Best coefficients so far\n",
    "    \n",
    "    for i in range(numIt):\n",
    "        lowestError = float('inf')\n",
    "        \n",
    "        # Try adjusting each coefficient\n",
    "        for j in range(n):\n",
    "            # Try both increasing and decreasing\n",
    "            for sign in [-1, 1]:\n",
    "                wsTest = ws.copy()\n",
    "                wsTest[j] += eps * sign  # Adjust by small step\n",
    "                \n",
    "                # Calculate error with new coefficients\n",
    "                yTest = xMat * wsTest\n",
    "                rssE = rss_error(yMat.A, yTest.A)\n",
    "                \n",
    "                # Keep if this reduces error\n",
    "                if rssE < lowestError:\n",
    "                    lowestError = rssE\n",
    "                    wsMax = wsTest\n",
    "        \n",
    "        # Update coefficients with best adjustment\n",
    "        ws = wsMax.copy()\n",
    "        returnMat[i, :] = ws.T\n",
    "    \n",
    "    return returnMat\n",
    "\n",
    "\n",
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"Residual Sum of Squares error\"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u524d\u5411\u9010\u6b65\u7ebf\u6027\u56de\u5f52 (qi\u00e1nxi\u00e0ng zh\u00fab\u00f9 xi\u00e0nx\u00ecng hu\u00edgu\u012b)** = \"forward stagewise linear regression\"\n",
    "- **\u6bcf\u6b21\u8fed\u4ee3\u9700\u8981\u8c03\u6574\u7684\u6b65\u957f (m\u011bi c\u00ec di\u00e9d\u00e0i x\u016by\u00e0o ti\u00e1ozh\u011bng de b\u00f9ch\u00e1ng)** = \"step size for adjustment needed in each iteration\"\n",
    "- **\u8fed\u4ee3\u6b21\u6570 (di\u00e9d\u00e0i c\u00ecsh\u00f9)** = \"number of iterations\"\n",
    "- **\u56de\u5f52\u7cfb\u6570\u77e9\u9635 (hu\u00edgu\u012b x\u00ecsh\u00f9 j\u01d4zh\u00e8n)** = \"regression coefficient matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Ascent for Logistic Regression (\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\n",
    "    Parameters:\n",
    "        dataMatIn - \u6570\u636e\u96c6\n",
    "        classLabels - \u6570\u636e\u6807\u7b7e\n",
    "    Returns:\n",
    "        weights.getA() - \u6c42\u5f97\u7684\u6743\u91cd\u6570\u7ec4(\u6700\u4f18\u53c2\u6570)\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(dataMatIn, classLabels, alpha=0.001, maxCycles=500):\n",
    "    \"\"\"\n",
    "    Function: Gradient Ascent Algorithm for Logistic Regression\n",
    "    \n",
    "    Maximizes the log-likelihood function by following the gradient direction.\n",
    "    Used for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        dataMatIn - Dataset (feature matrix including intercept)\n",
    "        classLabels - Data labels (binary: 0 or 1)\n",
    "        alpha - Learning rate (step size in gradient direction)\n",
    "        maxCycles - Maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        weights.getA() - Obtained weight array (optimal parameters)\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Initialize weights to 1\n",
    "        2. For each iteration:\n",
    "           - Calculate predictions: h = sigmoid(X * w)\n",
    "           - Calculate error: error = y - h\n",
    "           - Update weights: w = w + \u03b1 * X^T * error\n",
    "        3. Return final weights\n",
    "    \n",
    "    Why \"Ascent\" not \"Descent\"?\n",
    "        - We MAXIMIZE log-likelihood (go uphill)\n",
    "        - Gradient descent MINIMIZES cost (go downhill)\n",
    "        - Same algorithm, opposite direction\n",
    "    \"\"\"\n",
    "    dataMatrix = np.asmatrix(dataMatIn)\n",
    "    labelMat = np.asmatrix(classLabels).transpose()\n",
    "    m, n = np.shape(dataMatrix)  # m samples, n features\n",
    "    \n",
    "    weights = np.ones((n, 1))  # Initialize weights\n",
    "    \n",
    "    for k in range(maxCycles):\n",
    "        # Forward pass: calculate predictions\n",
    "        h = sigmoid(dataMatrix * weights)\n",
    "        \n",
    "        # Calculate error (gradient direction)\n",
    "        error = labelMat - h\n",
    "        \n",
    "        # Update weights (gradient ascent step)\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error\n",
    "    \n",
    "    return weights.getA()\n",
    "\n",
    "\n",
    "def sigmoid(inX):\n",
    "    \"\"\"Sigmoid activation function: \u03c3(x) = 1 / (1 + e^-x)\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-inX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5 (t\u012bd\u00f9 sh\u00e0ngsh\u0113ng su\u00e0nf\u01ce)** = \"gradient ascent algorithm\"\n",
    "- **\u6570\u636e\u96c6 (sh\u00f9j\u00f9 j\u00ed)** = \"dataset\"\n",
    "- **\u6570\u636e\u6807\u7b7e (sh\u00f9j\u00f9 bi\u0101oqi\u0101n)** = \"data labels\"\n",
    "- **\u6743\u91cd\u6570\u7ec4 (qu\u00e1nzh\u00f2ng sh\u00f9z\u01d4)** = \"weight array\"\n",
    "- **\u6700\u4f18\u53c2\u6570 (zu\u00ec y\u014du c\u0101nsh\u00f9)** = \"optimal parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Improved Stochastic Gradient Ascent (\u6539\u8fdb\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5)\n",
    "\n",
    "**Original Chinese Comments:**\n",
    "```python\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    '''\n",
    "    \u51fd\u6570\u8bf4\u660e:\u6539\u8fdb\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\n",
    "    Parameters:\n",
    "        dataMatrix - \u6570\u636e\u6570\u7ec4\n",
    "        classLabels - \u6570\u636e\u6807\u7b7e\n",
    "        numIter - \u8fed\u4ee3\u6b21\u6570\n",
    "    Returns:\n",
    "        weights - \u6c42\u5f97\u7684\u56de\u5f52\u7cfb\u6570\u6570\u7ec4(\u6700\u4f18\u53c2\u6570)\n",
    "    '''\n",
    "```\n",
    "\n",
    "**English Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_stochastic_gradient_ascent(dataMatrix, classLabels, numIter=150):\n",
    "    \"\"\"\n",
    "    Function: Improved Stochastic Gradient Ascent Algorithm\n",
    "    \n",
    "    Faster than batch gradient ascent. Updates weights after EACH sample\n",
    "    instead of after ALL samples. Includes adaptive learning rate and\n",
    "    random sampling for better convergence.\n",
    "    \n",
    "    Parameters:\n",
    "        dataMatrix - Data array (feature matrix)\n",
    "        classLabels - Data labels (binary classification)\n",
    "        numIter - Number of iterations (passes through dataset)\n",
    "    \n",
    "    Returns:\n",
    "        weights - Obtained regression coefficient array (optimal parameters)\n",
    "    \n",
    "    Improvements over basic SGD:\n",
    "        1. Adaptive learning rate: \u03b1 = 4/(1+j+i) + 0.01\n",
    "           - Decreases over time (helps convergence)\n",
    "           - Never reaches zero (continues making progress)\n",
    "           - Base rate 0.01 prevents premature stopping\n",
    "        \n",
    "        2. Random sample selection:\n",
    "           - Prevents cycles in weight updates\n",
    "           - Reduces correlation between consecutive updates\n",
    "           - Samples without replacement within each epoch\n",
    "    \n",
    "    Advantages:\n",
    "        - Much faster than batch gradient (especially for large datasets)\n",
    "        - Can escape local minima (due to randomness)\n",
    "        - Updates happen immediately (online learning)\n",
    "    \n",
    "    Disadvantages:\n",
    "        - More noisy convergence path\n",
    "        - May not reach exact optimum (oscillates around it)\n",
    "    \"\"\"\n",
    "    m, n = np.shape(dataMatrix)  # m samples, n features\n",
    "    weights = np.ones(n)  # Initialize weights to 1\n",
    "    \n",
    "    for j in range(numIter):  # Multiple passes through data\n",
    "        dataIndex = list(range(m))  # Track which samples not yet used\n",
    "        \n",
    "        for i in range(m):  # For each sample in this pass\n",
    "            # Adaptive learning rate: decreases but never reaches 0\n",
    "            alpha = 4 / (1.0 + j + i) + 0.01\n",
    "            \n",
    "            # Randomly select a sample (without replacement)\n",
    "            randIndex = int(np.random.uniform(0, len(dataIndex)))\n",
    "            \n",
    "            # Calculate prediction and error for THIS sample only\n",
    "            h = sigmoid(sum(dataMatrix[randIndex] * weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            \n",
    "            # Update weights based on this single sample\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            \n",
    "            # Remove this sample from available pool\n",
    "            del(dataIndex[randIndex])\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u6539\u8fdb\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5 (g\u01ceij\u00ecn de su\u00edj\u012b t\u012bd\u00f9 sh\u00e0ngsh\u0113ng su\u00e0nf\u01ce)** = \"improved stochastic gradient ascent algorithm\"\n",
    "- **\u6570\u636e\u6570\u7ec4 (sh\u00f9j\u00f9 sh\u00f9z\u01d4)** = \"data array\"\n",
    "- **\u56de\u5f52\u7cfb\u6570\u6570\u7ec4 (hu\u00edgu\u012b x\u00ecsh\u00f9 sh\u00f9z\u01d4)** = \"regression coefficient array\"\n",
    "\n",
    "**Key Formula:**\n",
    "- Learning rate formula: **\u03b1 = 4/(1+j+i) + 0.01**\n",
    "  - j = current epoch (iteration through full dataset)\n",
    "  - i = current sample within epoch\n",
    "  - This ensures \u03b1 decreases over time but stays above 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Methods {#statistics}\n",
    "\n",
    "### 4.1 Performance Metrics\n",
    "\n",
    "#### Residual Sum of Squares (RSS)\n",
    "\n",
    "**Original Chinese:** \u8bef\u5dee\u5927\u5c0f\u8bc4\u4ef7\u51fd\u6570\n",
    "\n",
    "**Translation:** Error evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_error(yArr, yHatArr):\n",
    "    \"\"\"\n",
    "    Residual Sum of Squares (RSS) - Error Evaluation Function\n",
    "    \n",
    "    Measures total squared difference between actual and predicted values.\n",
    "    Lower values = better model fit.\n",
    "    \n",
    "    Formula: RSS = \u03a3(y_i - \u0177_i)\u00b2\n",
    "    \n",
    "    Used for:\n",
    "        - Comparing different models\n",
    "        - Selecting hyperparameters\n",
    "        - Evaluating regression performance\n",
    "    \"\"\"\n",
    "    return ((yArr - yHatArr)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Preprocessing Methods\n",
    "\n",
    "#### Min-Max Normalization (\u5f52\u4e00\u5316)\n",
    "\n",
    "**Original Chinese:** \u5f52\u4e00\u5316\u7279\u5f81\u503c\n",
    "\n",
    "**Translation:** Normalize feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(dataSet):\n",
    "    \"\"\"\n",
    "    Min-Max Normalization: Scale features to [0, 1] range\n",
    "    \n",
    "    Formula: x_norm = (x - min) / (max - min)\n",
    "    \n",
    "    Used in:\n",
    "        - k-Nearest Neighbors (distance-based algorithms)\n",
    "        - Neural networks\n",
    "        - Algorithms sensitive to feature scale\n",
    "    \n",
    "    Why normalize?\n",
    "        - Prevents features with large values from dominating\n",
    "        - Makes all features contribute equally to distance\n",
    "        - Required when features have different units\n",
    "    \"\"\"\n",
    "    minVals = dataSet.min(0)  # Minimum value of each feature\n",
    "    maxVals = dataSet.max(0)  # Maximum value of each feature\n",
    "    ranges = maxVals - minVals  # Range of each feature\n",
    "    \n",
    "    normDataSet = (dataSet - minVals) / ranges\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train/Test Splitting\n",
    "\n",
    "**Original Chinese:** \u8bad\u7ec3\u96c6\u6d4b\u8bd5\u96c6\u5206\u79bb\n",
    "\n",
    "**Translation:** Training set and test set separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from colicLogRegres.py\n",
    "\n",
    "def train_test_split_example():\n",
    "    \"\"\"\n",
    "    Example: Proper train/test split methodology\n",
    "    \n",
    "    The repository uses separate files for training and testing:\n",
    "        - horseColicTraining.txt (training set - \u8bad\u7ec3\u96c6)\n",
    "        - horseColicTest.txt (test set - \u6d4b\u8bd5\u96c6)\n",
    "    \n",
    "    This prevents data leakage and provides honest performance estimates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    frTrain = open('horseColicTraining.txt')\n",
    "    trainingSet = []\n",
    "    trainingLabels = []\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    \n",
    "    # Train model\n",
    "    trainWeights = improved_stochastic_gradient_ascent(\n",
    "        np.array(trainingSet), trainingLabels, 500\n",
    "    )\n",
    "    \n",
    "    # Load test data (completely separate)\n",
    "    frTest = open('horseColicTest.txt')\n",
    "    errorCount = 0\n",
    "    numTestVec = 0\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        \n",
    "        prediction = classify_vector(np.array(lineArr), trainWeights)\n",
    "        if int(prediction) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    \n",
    "    # Calculate error rate (\u9519\u8bef\u7387)\n",
    "    errorRate = (float(errorCount) / numTestVec) * 100\n",
    "    print(f\"Error rate: {errorRate:.2f}%\")  # \u9519\u8bef\u7387\n",
    "    \n",
    "    return errorRate\n",
    "\n",
    "\n",
    "def classify_vector(inX, weights):\n",
    "    \"\"\"Binary classification using sigmoid threshold\"\"\"\n",
    "    prob = sigmoid(sum(inX * weights))\n",
    "    return 1.0 if prob > 0.5 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Notes:**\n",
    "- **\u8bad\u7ec3\u96c6 (x\u00f9nli\u00e0n j\u00ed)** = \"training set\"\n",
    "- **\u6d4b\u8bd5\u96c6 (c\u00e8sh\u00ec j\u00ed)** = \"test set\"\n",
    "- **\u9519\u8bef\u7387 (cu\u00f2w\u00f9 l\u01dc)** = \"error rate\"\n",
    "- **\u5206\u7c7b (f\u0113nl\u00e8i)** = \"classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Code Example with Translation {#examples}\n",
    "\n",
    "### Ridge Regression: Full Workflow\n",
    "\n",
    "This section shows a complete example with all Chinese comments translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Complete Ridge Regression Example with Full Translation\n",
    "\n",
    "def complete_ridge_regression_example():\n",
    "    \"\"\"\n",
    "    Complete example showing:\n",
    "    1. Data loading (\u6570\u636e\u52a0\u8f7d)\n",
    "    2. Standardization (\u6807\u51c6\u5316)\n",
    "    3. Ridge regression (\u5cad\u56de\u5f52)\n",
    "    4. Regularization path (\u6b63\u5219\u5316\u8def\u5f84)\n",
    "    5. Visualization (\u53ef\u89c6\u5316)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Ridge Regression Complete Example\")\n",
    "    print(\"\u5cad\u56de\u5f52\u5b8c\u6574\u793a\u4f8b\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load data (\u52a0\u8f7d\u6570\u636e)\n",
    "    print(\"\\n1. Loading data... (\u52a0\u8f7d\u6570\u636e...)\")\n",
    "    X, y = generate_synthetic_data()\n",
    "    print(f\"   Samples (\u6837\u672c\u6570): {len(X)}\")\n",
    "    print(f\"   Features (\u7279\u5f81\u6570): {len(X[0])}\")\n",
    "    \n",
    "    # Step 2: Standardize data (\u6807\u51c6\u5316\u6570\u636e)\n",
    "    print(\"\\n2. Standardizing data... (\u6807\u51c6\u5316\u6570\u636e...)\")\n",
    "    xMat = np.asmatrix(X)\n",
    "    yMat = np.asmatrix(y).T\n",
    "    \n",
    "    yMean = np.mean(yMat, axis=0)\n",
    "    yMat = yMat - yMean\n",
    "    xMeans = np.mean(xMat, axis=0)\n",
    "    xVar = np.var(xMat, axis=0)\n",
    "    xMat = (xMat - xMeans) / xVar\n",
    "    print(\"   Data standardized (\u6570\u636e\u5df2\u6807\u51c6\u5316)\")\n",
    "    \n",
    "    # Step 3: Test multiple lambda values (\u6d4b\u8bd5\u591a\u4e2alambda\u503c)\n",
    "    print(\"\\n3. Testing 30 lambda values... (\u6d4b\u8bd530\u4e2alambda\u503c...)\")\n",
    "    numTestPts = 30\n",
    "    wMat = np.zeros((numTestPts, np.shape(xMat)[1]))\n",
    "    lambda_values = []\n",
    "    \n",
    "    for i in range(numTestPts):\n",
    "        lam = np.exp(i - 10)  # Lambda from e^-10 to e^19\n",
    "        lambda_values.append(lam)\n",
    "        \n",
    "        # Ridge regression formula: w = (X^T X + \u03bbI)^-1 X^T y\n",
    "        xTx = xMat.T * xMat\n",
    "        denom = xTx + np.eye(np.shape(xMat)[1]) * lam\n",
    "        \n",
    "        if np.linalg.det(denom) != 0.0:\n",
    "            ws = denom.I * (xMat.T * yMat)\n",
    "            wMat[i, :] = ws.T\n",
    "    \n",
    "    print(f\"   Lambda range (\u03bb\u8303\u56f4): {min(lambda_values):.6f} to {max(lambda_values):.2f}\")\n",
    "    \n",
    "    # Step 4: Visualize regularization path (\u53ef\u89c6\u5316\u6b63\u5219\u5316\u8def\u5f84)\n",
    "    print(\"\\n4. Creating visualization... (\u521b\u5efa\u53ef\u89c6\u5316...)\")\n",
    "    plot_regularization_path(wMat, lambda_values)\n",
    "    \n",
    "    # Step 5: Show coefficient shrinkage (\u663e\u793a\u7cfb\u6570\u6536\u7f29)\n",
    "    print(\"\\n5. Coefficient shrinkage (\u7cfb\u6570\u6536\u7f29):\")\n",
    "    print(f\"   \u03bb = {lambda_values[0]:.6f} (low regularization):  {wMat[0, :]}\")\n",
    "    print(f\"   \u03bb = {lambda_values[15]:.6f} (medium regularization): {wMat[15, :]}\")\n",
    "    print(f\"   \u03bb = {lambda_values[29]:.2f} (high regularization):  {wMat[29, :]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Complete! (\u5b8c\u6210!)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\"Generate synthetic data for demonstration (\u751f\u6210\u793a\u4f8b\u6570\u636e)\"\"\"\n",
    "    np.random.seed(42)\n",
    "    m = 100  # Number of samples (\u6837\u672c\u6570)\n",
    "    n = 5    # Number of features (\u7279\u5f81\u6570)\n",
    "    \n",
    "    X = np.random.randn(m, n)\n",
    "    true_weights = np.array([1.5, -2.0, 0.5, 3.0, -1.0])\n",
    "    y = X @ true_weights + np.random.randn(m) * 0.5\n",
    "    \n",
    "    return X.tolist(), y.tolist()\n",
    "\n",
    "\n",
    "def plot_regularization_path(wMat, lambda_values):\n",
    "    \"\"\"Plot how coefficients change with regularization (\u7ed8\u5236\u7cfb\u6570\u53d8\u5316)\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(np.log(lambda_values), wMat)\n",
    "    ax.set_xlabel('log(\u03bb) - Regularization Strength (\u6b63\u5219\u5316\u5f3a\u5ea6)', fontsize=12)\n",
    "    ax.set_ylabel('Regression Coefficients (\u56de\u5f52\u7cfb\u6570)', fontsize=12)\n",
    "    ax.set_title('Ridge Regression Regularization Path\\n\u5cad\u56de\u5f52\u6b63\u5219\u5316\u8def\u5f84', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Add legend (\u6dfb\u52a0\u56fe\u4f8b)\n",
    "    ax.legend([f'Feature {i+1} (\u7279\u5f81{i+1})' for i in range(wMat.shape[1])],\n",
    "              loc='best', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   Regularization path plotted (\u6b63\u5219\u5316\u8def\u5f84\u5df2\u7ed8\u5236)\")\n",
    "\n",
    "\n",
    "# Run the complete example\n",
    "if __name__ == '__main__':\n",
    "    complete_ridge_regression_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. \u2705 **Complete translation** of all Chinese terms to English\n",
    "2. \u2705 **Translation dictionary** for quick reference\n",
    "3. \u2705 **Annotated code** with original Chinese comments and English translations\n",
    "4. \u2705 **Detailed explanations** of all algorithms and statistical methods\n",
    "5. \u2705 **Working examples** that you can run and modify\n",
    "\n",
    "### Key Translations to Remember:\n",
    "\n",
    "- **\u5cad\u56de\u5f52 (l\u01d0ng hu\u00edgu\u012b)** = Ridge Regression\n",
    "- **\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52 (j\u00fab\u00f9 ji\u0101qu\u00e1n xi\u00e0nx\u00ecng hu\u00edgu\u012b)** = Locally Weighted Linear Regression (LWLR)\n",
    "- **\u524d\u5411\u9010\u6b65\u7ebf\u6027\u56de\u5f52 (qi\u00e1nxi\u00e0ng zh\u00fab\u00f9 xi\u00e0nx\u00ecng hu\u00edgu\u012b)** = Forward Stagewise Linear Regression\n",
    "- **\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5 (t\u012bd\u00f9 sh\u00e0ngsh\u0113ng su\u00e0nf\u01ce)** = Gradient Ascent Algorithm\n",
    "- **\u6570\u636e\u6807\u51c6\u5316 (sh\u00f9j\u00f9 bi\u0101ozh\u01d4nhu\u00e0)** = Data Standardization\n",
    "- **\u8bad\u7ec3\u96c6 (x\u00f9nli\u00e0n j\u00ed)** = Training Set\n",
    "- **\u6d4b\u8bd5\u96c6 (c\u00e8sh\u00ec j\u00ed)** = Test Set\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Review the translated algorithms\n",
    "2. Run the example code cells\n",
    "3. Proceed to the implementation notebooks\n",
    "4. Apply these methods to your own data\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/enzodata3-blip/Task4  \n",
    "**Original Source:** https://github.com/Jack-Cherish/Machine-Learning  \n",
    "**Created:** 2026-02-09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}